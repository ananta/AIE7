{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ðŸ¤ Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "- ðŸ¤ Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ðŸ¤ Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "3fa78560-393c-4ee5-b871-9886bf0d70f4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkla2fpx28QK",
        "outputId": "52d7ad22-fcb1-4abe-853b-216c55a12650"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b69df90a-b4e1-4ddb-9de0-882d98b68ab2"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE7 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain-community/tree/main/libs/community) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Tavily Search Results](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/tavily_search/tool.py)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain-community/blob/main/libs/community/langchain_community/tools/arxiv/tool.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "from langchain_core.tools import Tool\n",
        "\n",
        "\n",
        "def helloworld(name: str) -> str:\n",
        "    return f\"Hello ${name}\"\n",
        "\n",
        "hello_tool = Tool(\n",
        "    name = \"helloworld\",\n",
        "    func = helloworld,\n",
        "    description = \"Greet the user\"\n",
        ")\n",
        "\n",
        "tavily_tool = TavilySearchResults(max_results=5)\n",
        "\n",
        "tool_belt = [\n",
        "    tavily_tool,\n",
        "    ArxivQueryRun(),\n",
        "    hello_tool\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4.1-nano\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### â“ Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "\n",
        "Looking at our custom tool implementation, we have something like this;\n",
        "```\n",
        "hello_tool = Tool(\n",
        "    name = \"helloworld\",\n",
        "    func = helloworld,\n",
        "    description = \"Greet the user\"\n",
        ")\n",
        "```\n",
        "The model uses the description of the tool to determine what the tool does in an agentic workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vF4_lgtmQNo",
        "outputId": "a4384377-8f7a-415f-be1b-fee6169cb101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11e0046e0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGCbaYqRnmiw",
        "outputId": "5351807c-2ac7-4316-a3a3-878abeacd114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11e0046e0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BZgb81VQf9o",
        "outputId": "73a07c15-5f0b-40f2-b033-38b57d056dd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11e0046e0>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvcgbHf1rIXZ",
        "outputId": "45d4bdd6-d6bb-4a1d-bb79-cad43c130bf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11e0046e0>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "simple_agent_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### â“ Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "Yes, based on the langgraph docs, we do have a `graph_recursion_limit=25`. But, based on our requirements, we can either increase or deecrease this amount.\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "We can keep track of the aggregator value in our state, so that with each call, we can increase the value of the aggregator if it's  less than the max limit. And if it's equal or more, we'll redirect the flow to END."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "5eeedfae-089d-496e-e71f-071939fa5832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_l1PvMqPG8djHKYIbonOUiHke', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 188, 'total_tokens': 211, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BtTG893iqnzAxe1gIxLvYIBd3ZLQe', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--793ec0c8-b3d6-45b1-aa2c-740430158e5c-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current captain of the Winnipeg Jets'}, 'id': 'call_l1PvMqPG8djHKYIbonOUiHke', 'type': 'tool_call'}], usage_metadata={'input_tokens': 188, 'output_tokens': 23, 'total_tokens': 211, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='[{\"title\": \"Winnipeg Jets - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Winnipeg_Jets\", \"content\": \"|  v  t  e  Winnipeg Jets | |\\\\n| --- | --- |\\\\n|  Formerly the Atlanta Thrashers  Founded in 1999  Based in Winnipeg, Manitoba | |\\\\n| Franchise |  Team  General managers  Coaches  Players  Captains  Draft picks   + expansion draft  Seasons  Current season |\\\\n| History |  Records  Award winners  Broadcasters |\\\\n| Personnel | Owner(s)  True North Sports & Entertainment (Mark Chipman, chairman)  General manager  Kevin Cheveldayoff  Head coach  Scott Arniel  Team captain  Adam Lowry  Current roster | [...] the draft lottery, which they used to select Finnish prospect Patrik Laine. Later that summer, the team appointed Blake Wheeler as their new captain. [...] In the 2021â€“22 season, the Jets finished a disappointing sixth in the Central Division, missing the playoffs. At the start of the 2022â€“23 season, forward Blake Wheeler was stripped of the team captaincy. The Jets then clinched the 2023 playoffs at the end of the regular season, but were defeated by the eventual Stanley Cup champion Vegas Golden Knights in five games in the first round. Before the start of the 2023â€“24 season, forward Adam Lowry was appointed team captain. The Jets then clinched\", \"score\": 0.885404}, {\"title\": \"Adam Lowry named Jets captain | Winnipeg Jets - NHL.com\", \"url\": \"https://www.nhl.com/jets/news/adam-lowry-named-jets-captain\", \"content\": \"That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the third captain in franchise history since the team moved here from Atlanta. He follows Andrew Ladd and Blake Wheeler who served as captain for five and six years respectively.\\\\n\\\\nÃ¢\\x80\\x9cWhen I found out, I was pretty excited, almost a little speechless. ItÃ¢\\x80\\x99s something growing up you kind of can dream about and something that seems almost unattainable,Ã¢\\x80\\x9d said Lowry. [...] Ã¢\\x80\\x9cHeÃ¢\\x80\\x99s a true professional, he has total respect from every player on the team, every player around the league and certainly from the coaching staff as well. We just feel at this point itÃ¢\\x80\\x99s the right time to name Adam as our captain.Ã¢\\x80\\x9d [...] Ã¢\\x80\\x9cGetting to be a captain of a Canadian NHL team is pretty special and something IÃ¢\\x80\\x99m really looking forward too.Ã¢\\x80\\x9d\", \"score\": 0.8449346}, {\"title\": \"Lowry named Jets captain, replaces Wheeler | NHL.com\", \"url\": \"https://www.nhl.com/news/adam-lowry-named-winnipeg-captain\", \"content\": \"NHL logo\\\\nNHL logo\\\\n\\\\n# Lowry named Jets captain, replaces Wheeler\\\\n\\\\n30-year-old forward entering 10th season with Winnipeg\\\\n\\\\nLowry_Jets\\\\n\\\\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday.\\\\n\\\\nThe 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine-season NHL career with Winnipeg. [...] Lowry replaces Blake Wheeler, who was removed as captain Sept. 16, 2022, and signed with the New York Rangers after having his contract bought out this offseason. The Jets opted for three alternate captains last season; Lowry, forward Mark Scheifele and defenseman Josh Morrissey. Coach Rick Bowness said Scheifele and Morrissey will remain alternate captains. [...] Lowry said he\\'s learned from the captains he\\'s played with in Winnipeg, Wheeler and Andrew Ladd, and believes the important thing is staying true to the player he\\'s always been.\", \"score\": 0.84027237}, {\"title\": \"Winnipeg Jets Captain To Miss Start of Next Season - Sports Illustrated\", \"url\": \"https://www.si.com/onsi/breakaway/news-feed-page/winnipeg-jets-adam-lowry-miss-start-next-season\", \"content\": \"The Winnipeg Jets fell short of expectations after winning the 2024-25 PresidentÃ¢\\x80\\x99s Trophy, but they are retooling and looking forward to next season. Despite the high hopes, the Jets will likely have to start the 2025-26 season without their captain.\\\\n\\\\nThe Jets announced that captain Adam Lowry underwent a successful hip surgery and is expected to be sidelined for five to six months. Lowry played 73 games during the 2024-25 regular season and didnÃ¢\\x80\\x99t miss a single playoff game. [...] The following is a statement from the Winnipeg Jets Hockey Club regarding Captain Adam Lowry: pic.twitter.com/t6S7wajrKq\\\\n\\\\nLowry never showed signs of needing postseason surgery, averaging almost 17:30 in ice time throughout the playoffs.\\\\n\\\\nIn 13 playoff games this season, Lowry scored four goals, including the double overtime game-winner in Game 7 of their opening-round series against the St. Louis Blues.\", \"score\": 0.8361405}, {\"title\": \"Jets\\' Captain Adam Lowry Speaks on Team\\'s Playoff Performance ...\", \"url\": \"https://thehockeywriters.com/winnipeg-jets-adam-lowry-end-of-season-interview-2025/\", \"content\": \"The Hockey Writers\\\\nThe Hockey Writers\\\\n\\\\n# Jetsâ€™ Captain Adam Lowry Speaks on Teamâ€™s Playoff Performance & Being a Jet for Life\\\\n\\\\nThe disappointment of their 2025 playoff defeat hasnâ€™t faded for Winnipeg Jetsâ€™ captain Adam Lowry, but heâ€™s determined to lead his team to glory in the future.\\\\n\\\\n## â€œWe Felt Like We Had a Team That Should and Could Still Be Playingâ€ [...] So real: Jets captain Adam Lowry goes into penalty box to console teammate Mark Scheifele, who was 8 secs from getting out when Dallas scored series-winning power play goal in OT to end Winnipegâ€™s season. Scheifele was playing hours after his dad died.  \\\\n  \\\\nStanley Cup playoffs, man pic.twitter.com/8u5xeSQp8m\\\\n\\\\nLowry only has one year left on the five-year deal he signed in 2021, but at 32 years old and the longest-tenured Jet along with Scheifele, he doesnâ€™t plan on going anywhere. [...] ## â€œThatâ€™s Kind of How I Picture Myself: As a Winnipeg Jet For Lifeâ€\\\\n\\\\nLowry had 16 goals and 18 assists for 34 points in 73 games this season and four goals in 13 playoff games, including the double overtime Game 7 game winner to lift the Jets to a first-round series victory over the Blues. Just as importantly, he became an even more effective leader in his second season as captain and continued to lead by example with his work ethic and genuine nature.\", \"score\": 0.7823471}]', name='tavily_search_results_json', id='0aceb9fb-d7c3-496e-be22-fb37a4cad58d', tool_call_id='call_l1PvMqPG8djHKYIbonOUiHke', artifact={'query': 'current captain of the Winnipeg Jets', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://en.wikipedia.org/wiki/Winnipeg_Jets', 'title': 'Winnipeg Jets - Wikipedia', 'content': '|  v  t  e  Winnipeg Jets | |\\n| --- | --- |\\n|  Formerly the Atlanta Thrashers  Founded in 1999  Based in Winnipeg, Manitoba | |\\n| Franchise |  Team  General managers  Coaches  Players  Captains  Draft picks   + expansion draft  Seasons  Current season |\\n| History |  Records  Award winners  Broadcasters |\\n| Personnel | Owner(s)  True North Sports & Entertainment (Mark Chipman, chairman)  General manager  Kevin Cheveldayoff  Head coach  Scott Arniel  Team captain  Adam Lowry  Current roster | [...] the draft lottery, which they used to select Finnish prospect Patrik Laine. Later that summer, the team appointed Blake Wheeler as their new captain. [...] In the 2021â€“22 season, the Jets finished a disappointing sixth in the Central Division, missing the playoffs. At the start of the 2022â€“23 season, forward Blake Wheeler was stripped of the team captaincy. The Jets then clinched the 2023 playoffs at the end of the regular season, but were defeated by the eventual Stanley Cup champion Vegas Golden Knights in five games in the first round. Before the start of the 2023â€“24 season, forward Adam Lowry was appointed team captain. The Jets then clinched', 'score': 0.885404, 'raw_content': None}, {'url': 'https://www.nhl.com/jets/news/adam-lowry-named-jets-captain', 'title': 'Adam Lowry named Jets captain | Winnipeg Jets - NHL.com', 'content': 'That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the third captain in franchise history since the team moved here from Atlanta. He follows Andrew Ladd and Blake Wheeler who served as captain for five and six years respectively.\\n\\nÃ¢\\x80\\x9cWhen I found out, I was pretty excited, almost a little speechless. ItÃ¢\\x80\\x99s something growing up you kind of can dream about and something that seems almost unattainable,Ã¢\\x80\\x9d said Lowry. [...] Ã¢\\x80\\x9cHeÃ¢\\x80\\x99s a true professional, he has total respect from every player on the team, every player around the league and certainly from the coaching staff as well. We just feel at this point itÃ¢\\x80\\x99s the right time to name Adam as our captain.Ã¢\\x80\\x9d [...] Ã¢\\x80\\x9cGetting to be a captain of a Canadian NHL team is pretty special and something IÃ¢\\x80\\x99m really looking forward too.Ã¢\\x80\\x9d', 'score': 0.8449346, 'raw_content': None}, {'url': 'https://www.nhl.com/news/adam-lowry-named-winnipeg-captain', 'title': 'Lowry named Jets captain, replaces Wheeler | NHL.com', 'content': \"NHL logo\\nNHL logo\\n\\n# Lowry named Jets captain, replaces Wheeler\\n\\n30-year-old forward entering 10th season with Winnipeg\\n\\nLowry_Jets\\n\\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday.\\n\\nThe 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine-season NHL career with Winnipeg. [...] Lowry replaces Blake Wheeler, who was removed as captain Sept. 16, 2022, and signed with the New York Rangers after having his contract bought out this offseason. The Jets opted for three alternate captains last season; Lowry, forward Mark Scheifele and defenseman Josh Morrissey. Coach Rick Bowness said Scheifele and Morrissey will remain alternate captains. [...] Lowry said he's learned from the captains he's played with in Winnipeg, Wheeler and Andrew Ladd, and believes the important thing is staying true to the player he's always been.\", 'score': 0.84027237, 'raw_content': None}, {'url': 'https://www.si.com/onsi/breakaway/news-feed-page/winnipeg-jets-adam-lowry-miss-start-next-season', 'title': 'Winnipeg Jets Captain To Miss Start of Next Season - Sports Illustrated', 'content': 'The Winnipeg Jets fell short of expectations after winning the 2024-25 PresidentÃ¢\\x80\\x99s Trophy, but they are retooling and looking forward to next season. Despite the high hopes, the Jets will likely have to start the 2025-26 season without their captain.\\n\\nThe Jets announced that captain Adam Lowry underwent a successful hip surgery and is expected to be sidelined for five to six months. Lowry played 73 games during the 2024-25 regular season and didnÃ¢\\x80\\x99t miss a single playoff game. [...] The following is a statement from the Winnipeg Jets Hockey Club regarding Captain Adam Lowry: pic.twitter.com/t6S7wajrKq\\n\\nLowry never showed signs of needing postseason surgery, averaging almost 17:30 in ice time throughout the playoffs.\\n\\nIn 13 playoff games this season, Lowry scored four goals, including the double overtime game-winner in Game 7 of their opening-round series against the St. Louis Blues.', 'score': 0.8361405, 'raw_content': None}, {'url': 'https://thehockeywriters.com/winnipeg-jets-adam-lowry-end-of-season-interview-2025/', 'title': \"Jets' Captain Adam Lowry Speaks on Team's Playoff Performance ...\", 'content': 'The Hockey Writers\\nThe Hockey Writers\\n\\n# Jetsâ€™ Captain Adam Lowry Speaks on Teamâ€™s Playoff Performance & Being a Jet for Life\\n\\nThe disappointment of their 2025 playoff defeat hasnâ€™t faded for Winnipeg Jetsâ€™ captain Adam Lowry, but heâ€™s determined to lead his team to glory in the future.\\n\\n## â€œWe Felt Like We Had a Team That Should and Could Still Be Playingâ€ [...] So real: Jets captain Adam Lowry goes into penalty box to console teammate Mark Scheifele, who was 8 secs from getting out when Dallas scored series-winning power play goal in OT to end Winnipegâ€™s season. Scheifele was playing hours after his dad died.  \\n  \\nStanley Cup playoffs, man pic.twitter.com/8u5xeSQp8m\\n\\nLowry only has one year left on the five-year deal he signed in 2021, but at 32 years old and the longest-tenured Jet along with Scheifele, he doesnâ€™t plan on going anywhere. [...] ## â€œThatâ€™s Kind of How I Picture Myself: As a Winnipeg Jet For Lifeâ€\\n\\nLowry had 16 goals and 18 assists for 34 points in 73 games this season and four goals in 13 playoff games, including the double overtime Game 7 game winner to lift the Jets to a first-round series victory over the Blues. Just as importantly, he became an even more effective leader in his second season as captain and continued to lead by example with his work ethic and genuine nature.', 'score': 0.7823471, 'raw_content': None}], 'response_time': 3.48})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The current captain of the Winnipeg Jets is Adam Lowry.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 1715, 'total_tokens': 1728, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BtTGC4VYgXaLlppgaAqay469LyUP1', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f748c245-450f-46f3-999a-ffe7672422c7-0', usage_metadata={'input_tokens': 1715, 'output_tokens': 13, 'total_tokens': 1728, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "ff009536-d281-4a56-c126-9cd245352bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7MIdHo8IFK1RlfMXkGA2jogY', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_9TgI5XlMflFrA35Q7da9aFLd', 'function': {'arguments': '{\"query\": \"latest Tweet of the first author of QLoRA\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_5iY1dSinISEsiUIzk3ZF2wps', 'function': {'arguments': '{\"query\": \"latest Tweet of the second author of QLoRA\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_0eeIMsjmWOK47kTneIiujqfs', 'function': {'arguments': '{\"query\": \"latest Tweet of the third author of QLoRA\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 204, 'total_tokens': 317, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BtTOe9fSwvyUdSqSmz4JTVMdXVQhU', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--97049ab0-f2d4-46fa-92b2-123bf2ff5a9f-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_7MIdHo8IFK1RlfMXkGA2jogY', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'latest Tweet of the first author of QLoRA'}, 'id': 'call_9TgI5XlMflFrA35Q7da9aFLd', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'latest Tweet of the second author of QLoRA'}, 'id': 'call_5iY1dSinISEsiUIzk3ZF2wps', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'latest Tweet of the third author of QLoRA'}, 'id': 'call_0eeIMsjmWOK47kTneIiujqfs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 204, 'output_tokens': 113, 'total_tokens': 317, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2025-02-05\\nTitle: Resource-Efficient & Effective Code Summarization\\nAuthors: Saima Afrin, Joseph Call, Khai-Nguyen Nguyen, Oscar Chaparro, Antonio Mastropaolo\\nSummary: Code Language Models (CLMs) have demonstrated high effectiveness in\\nautomating software engineering tasks such as bug fixing, code generation, and\\ncode documentation. This ', name='arxiv', id='83a4c144-990d-4f33-bf60-e954c8263840', tool_call_id='call_7MIdHo8IFK1RlfMXkGA2jogY'), ToolMessage(content='[{\"title\": \"QLoRA: A new way to finetune LLMs | by Darius Singh | Medium\", \"url\": \"https://medium.com/@dariussingh/qlora-a-new-way-to-finetune-llms-4a5ff292903d\", \"content\": \"Sign up\\\\n\\\\nSign in\\\\n\\\\nSign up\\\\n\\\\nSign in\\\\n\\\\n# QLoRA: A new way to finetune LLMs\\\\n\\\\nDarius Singh\\\\n\\\\n--\\\\n\\\\nListen\\\\n\\\\nShare\\\\n\\\\nQLoRA: Efficient Finetuning of Quantized LLMs by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman and Luke Zettlemoyer. [...] QLoRA is an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pre-trained language model into Low-Rank Adapters (LoRA).\\\\n\\\\n# Innovations\\\\n\\\\nQLoRA introduces the following innovations to reduce memory usage without sacrificing performance:  \\\\n1) 4-bit NormalFloat (NF4):\\\\n\\\\n2) Double Quantization (DQ): [...] 3) Paged Optimizers:\\\\n\\\\n# Key Insights\\\\n\\\\nKey insights from experiments:\\\\n\\\\nDid I miss anything? What are some LLM papers I should summarize next? Letâ€™s continue the conversation in the comments and learn from each other.\\\\n\\\\nPaper: \\\\n\\\\nGithub:  and \\\\n\\\\n--\\\\n\\\\n--\\\\n\\\\nDarius Singh\\\\nDarius Singh\\\\n\\\\n## Written by Darius Singh\\\\n\\\\nMachine Learning Entrepreneur\\\\n\\\\n## No responses yet\\\\n\\\\nHelp\\\\n\\\\nStatus\\\\n\\\\nAbout\\\\n\\\\nCareers\\\\n\\\\nPress\\\\n\\\\nBlog\\\\n\\\\nPrivacy\\\\n\\\\nRules\\\\n\\\\nTerms\\\\n\\\\nText to speech\", \"score\": 0.36643156}, {\"title\": \"artidoro/qlora - Efficient Finetuning of Quantized LLMs - GitHub\", \"url\": \"https://github.com/artidoro/qlora\", \"content\": \"There was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n### Stars\\\\n\\\\n### Watchers\\\\n\\\\n### Forks\\\\n\\\\n## Releases\\\\n\\\\n## Packages 0\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n## Contributors 16\\\\n\\\\n@artidoro\\\\n@TimDettmers\\\\n@tobi\\\\n@KKcorps\\\\n@Birch-san\\\\n@pmysl\\\\n@bubundas17\\\\n@abhilash1910\\\\n@Qubitium\\\\n@dameikle\\\\n@ranchlai\\\\n@steremma\\\\n@muelletm\\\\n@lhoestq\\\\n\\\\n## Languages [...] ## Latest commit\\\\n\\\\n## History\\\\n\\\\n## Repository files navigation\\\\n\\\\n# QLoRA: Efficient Finetuning of Quantized LLMs\\\\n\\\\n| Paper | Adapter Weights | Demo |\\\\n\\\\nThis repo supports the paper \\\\\"QLoRA: Efficient Finetuning of Quantized LLMs\\\\\", an effort to democratize access to LLM research.\\\\n\\\\nQLoRA uses bitsandbytes for quantization and is integrated with Hugging Face\\'s PEFT and transformers libraries. QLoRA was developed by members of the University of Washington\\'s UW NLP group.\\\\n\\\\n## Updates\\\\n\\\\n## Overview [...] You can specify the path to your dataset using the `--dataset` argument. If the `--dataset_format` argument is not set, it will default to the Alpaca format. Here are a few examples:\\\\n\\\\n`--dataset`\\\\n`--dataset_format`\\\\n\\\\n### Multi GPU\\\\n\\\\nMulti GPU training and inference work out-of-the-box with Hugging Face\\'s Accelerate. Note that the `per_device_train_batch_size` and `per_device_eval_batch_size` arguments are global batch sizes unlike what their name suggest.\", \"score\": 0.27467275}, {\"title\": \"[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - arXiv\", \"url\": \"https://arxiv.org/abs/2305.14314\", \"content\": \"close this message\\\\narXiv smileybones\\\\n\\\\n## arXiv Is Hiring a DevOps Engineer\\\\n\\\\nWork on one of the world\\'s most important websites and make an impact on open science.\\\\n\\\\nCornell University\\\\n\\\\narXiv Is Hiring a DevOps Engineer\\\\n\\\\narxiv logo\\\\n\\\\nHelp | Advanced Search\\\\n\\\\narXiv logo\\\\nCornell University Logo\\\\n\\\\n## quick links\\\\n\\\\n# Computer Science > Machine Learning\\\\n\\\\n# Title:QLoRA: Efficient Finetuning of Quantized LLMs [...] Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\\\n\\\\nHave an idea for a project that will add value for arXiv\\'s community? Learn more about arXivLabs.\\\\n\\\\narXiv Operational Status   \\\\nGet status notifications via\\\\nemail\\\\nor slack [...] |  |  |\\\\n| --- | --- |\\\\n| Comments: | Extended NeurIPS submission |\\\\n| Subjects: | Machine Learning (cs.LG) |\\\\n| Cite as: | arXiv:2305.14314 [cs.LG] |\\\\n|  | (or  arXiv:2305.14314v1 [cs.LG] for this version) |\\\\n|  |  Focus to learn more  arXiv-issued DOI via DataCite |\\\\n\\\\n## Submission history\\\\n\\\\n## Access Paper:\\\\n\\\\nlicense icon\\\\n\\\\n### References & Citations\\\\n\\\\n### 6 blog links\\\\n\\\\n## BibTeX formatted citation\\\\n\\\\n### Bookmark\\\\n\\\\nBibSonomy logo\\\\nReddit logo\\\\n\\\\n# Bibliographic and Citation Tools\", \"score\": 0.18207656}, {\"title\": \"NeurIPS Poster QLoRA: Efficient Finetuning of Quantized LLMs\", \"url\": \"https://neurips.cc/virtual/2023/poster/71815\", \"content\": \"We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only [...] We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only [...] evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\", \"score\": 0.17835435}, {\"title\": \"QLoRA: Efficient Finetuning of Quantized LLMs | Hacker News\", \"url\": \"https://news.ycombinator.com/item?id=36064568\", \"content\": \"Altman\\'s push for regulatory capture makes so much sense given how fast this field is going. Open models you can run on regular hardware are\", \"score\": 0.1548358}]', name='tavily_search_results_json', id='dc8aad50-b3f5-4ee1-8098-fe8420085917', tool_call_id='call_9TgI5XlMflFrA35Q7da9aFLd', artifact={'query': 'latest Tweet of the first author of QLoRA', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://medium.com/@dariussingh/qlora-a-new-way-to-finetune-llms-4a5ff292903d', 'title': 'QLoRA: A new way to finetune LLMs | by Darius Singh | Medium', 'content': 'Sign up\\n\\nSign in\\n\\nSign up\\n\\nSign in\\n\\n# QLoRA: A new way to finetune LLMs\\n\\nDarius Singh\\n\\n--\\n\\nListen\\n\\nShare\\n\\nQLoRA: Efficient Finetuning of Quantized LLMs by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman and Luke Zettlemoyer. [...] QLoRA is an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pre-trained language model into Low-Rank Adapters (LoRA).\\n\\n# Innovations\\n\\nQLoRA introduces the following innovations to reduce memory usage without sacrificing performance:  \\n1) 4-bit NormalFloat (NF4):\\n\\n2) Double Quantization (DQ): [...] 3) Paged Optimizers:\\n\\n# Key Insights\\n\\nKey insights from experiments:\\n\\nDid I miss anything? What are some LLM papers I should summarize next? Letâ€™s continue the conversation in the comments and learn from each other.\\n\\nPaper: \\n\\nGithub:  and \\n\\n--\\n\\n--\\n\\nDarius Singh\\nDarius Singh\\n\\n## Written by Darius Singh\\n\\nMachine Learning Entrepreneur\\n\\n## No responses yet\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nRules\\n\\nTerms\\n\\nText to speech', 'score': 0.36643156, 'raw_content': None}, {'url': 'https://github.com/artidoro/qlora', 'title': 'artidoro/qlora - Efficient Finetuning of Quantized LLMs - GitHub', 'content': 'There was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n### Stars\\n\\n### Watchers\\n\\n### Forks\\n\\n## Releases\\n\\n## Packages 0\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n## Contributors 16\\n\\n@artidoro\\n@TimDettmers\\n@tobi\\n@KKcorps\\n@Birch-san\\n@pmysl\\n@bubundas17\\n@abhilash1910\\n@Qubitium\\n@dameikle\\n@ranchlai\\n@steremma\\n@muelletm\\n@lhoestq\\n\\n## Languages [...] ## Latest commit\\n\\n## History\\n\\n## Repository files navigation\\n\\n# QLoRA: Efficient Finetuning of Quantized LLMs\\n\\n| Paper | Adapter Weights | Demo |\\n\\nThis repo supports the paper \"QLoRA: Efficient Finetuning of Quantized LLMs\", an effort to democratize access to LLM research.\\n\\nQLoRA uses bitsandbytes for quantization and is integrated with Hugging Face\\'s PEFT and transformers libraries. QLoRA was developed by members of the University of Washington\\'s UW NLP group.\\n\\n## Updates\\n\\n## Overview [...] You can specify the path to your dataset using the `--dataset` argument. If the `--dataset_format` argument is not set, it will default to the Alpaca format. Here are a few examples:\\n\\n`--dataset`\\n`--dataset_format`\\n\\n### Multi GPU\\n\\nMulti GPU training and inference work out-of-the-box with Hugging Face\\'s Accelerate. Note that the `per_device_train_batch_size` and `per_device_eval_batch_size` arguments are global batch sizes unlike what their name suggest.', 'score': 0.27467275, 'raw_content': None}, {'url': 'https://arxiv.org/abs/2305.14314', 'title': '[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - arXiv', 'content': \"close this message\\narXiv smileybones\\n\\n## arXiv Is Hiring a DevOps Engineer\\n\\nWork on one of the world's most important websites and make an impact on open science.\\n\\nCornell University\\n\\narXiv Is Hiring a DevOps Engineer\\n\\narxiv logo\\n\\nHelp | Advanced Search\\n\\narXiv logo\\nCornell University Logo\\n\\n## quick links\\n\\n# Computer Science > Machine Learning\\n\\n# Title:QLoRA: Efficient Finetuning of Quantized LLMs [...] Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\\n\\narXiv Operational Status   \\nGet status notifications via\\nemail\\nor slack [...] |  |  |\\n| --- | --- |\\n| Comments: | Extended NeurIPS submission |\\n| Subjects: | Machine Learning (cs.LG) |\\n| Cite as: | arXiv:2305.14314 [cs.LG] |\\n|  | (or  arXiv:2305.14314v1 [cs.LG] for this version) |\\n|  |  Focus to learn more  arXiv-issued DOI via DataCite |\\n\\n## Submission history\\n\\n## Access Paper:\\n\\nlicense icon\\n\\n### References & Citations\\n\\n### 6 blog links\\n\\n## BibTeX formatted citation\\n\\n### Bookmark\\n\\nBibSonomy logo\\nReddit logo\\n\\n# Bibliographic and Citation Tools\", 'score': 0.18207656, 'raw_content': None}, {'url': 'https://neurips.cc/virtual/2023/poster/71815', 'title': 'NeurIPS Poster QLoRA: Efficient Finetuning of Quantized LLMs', 'content': 'We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only [...] We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only [...] evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.', 'score': 0.17835435, 'raw_content': None}, {'url': 'https://news.ycombinator.com/item?id=36064568', 'title': 'QLoRA: Efficient Finetuning of Quantized LLMs | Hacker News', 'content': \"Altman's push for regulatory capture makes so much sense given how fast this field is going. Open models you can run on regular hardware are\", 'score': 0.1548358, 'raw_content': None}], 'response_time': 3.83}), ToolMessage(content='[{\"title\": \"Dettmers, T., Pagnoni, A., Holtzman, A. and Zettlemoyer, L. (2023 ...\", \"url\": \"https://www.scirp.org/reference/referencespapers?referenceid=3652690\", \"content\": \"Dettmers, T., Pagnoni, A., Holtzman, A. and Zettlemoyer, L. (2023) QLoRA: Efficient Finetuning of Quantized LLMs.\\\\n\\\\nhas been cited by the following article:\\\\n\\\\nTITLE:\\\\nSmaller & Smarter: Score-Driven Network Chaining of Smaller Language Models\\\\n\\\\nAUTHORS: \\\\nGunika Dhingra, Siddansh Chawla, Vijay K. Madisetti, Arshdeep Bahga [...] #### Journals by Subject\\\\n\\\\n#### Publish with us\\\\n\\\\n|  |  |  |  |\\\\n| --- | --- | --- | --- |\\\\n| Twitter | Facebook | Linkedin | Weibo |\\\\n\\\\nTwitter\\\\nFacebook\\\\nLinkedin\\\\nWeibo\\\\n\\\\n|  |  |\\\\n| --- | --- |\\\\n|  | customer@scirp.org |\\\\n| WhatsApp | +86 18163351462(WhatsApp) |\\\\n| Click here to send a message to me | 1655362766 |\\\\n|  |  |\\\\n|  | Paper Publishing WeChat |\\\\n\\\\nWhatsApp\\\\nClick here to send a message to me\\\\n\\\\n#### Article citationsMore>> [...] |  |  |  |  |\\\\n| --- | --- | --- | --- |\\\\n| Twitter | Facebook | Linkedin | Weibo |\\\\n\\\\nTwitter\\\\nFacebook\\\\nLinkedin\\\\nWeibo\\\\n\\\\n|  |  |\\\\n| --- | --- |\\\\n|  | customer@scirp.org |\\\\n| WhatsApp | +86 18163351462(WhatsApp) |\\\\n| Click here to send a message to me | 1655362766 |\\\\n|  |  |\\\\n|  | Paper Publishing WeChat |\\\\n\\\\nWhatsApp\\\\nClick here to send a message to me\\\\n\\\\n#### Home\\\\n\\\\n#### About SCIRP\\\\n\\\\n#### Service\\\\n\\\\n#### Policies\", \"score\": 0.51822984}, {\"title\": \"#QLORA - Search / X\", \"url\": \"https://twitter.com/search?q=%23QLORA&src=hashtag_click\", \"content\": \"... Author: Piyush Thakur #AIforHealthcare #BrainTumorDetection #PaliGemma2 #HuggingFace #Transformers #MedicalAI #VisionLanguageModels #QLoRA #PyImageSearch.\", \"score\": 0.4921643}, {\"title\": \"artidoro/qlora - Efficient Finetuning of Quantized LLMs - GitHub\", \"url\": \"https://github.com/artidoro/qlora\", \"content\": \"There was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n### Stars\\\\n\\\\n### Watchers\\\\n\\\\n### Forks\\\\n\\\\n## Releases\\\\n\\\\n## Packages 0\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n## Contributors 16\\\\n\\\\n@artidoro\\\\n@TimDettmers\\\\n@tobi\\\\n@KKcorps\\\\n@Birch-san\\\\n@pmysl\\\\n@bubundas17\\\\n@abhilash1910\\\\n@Qubitium\\\\n@dameikle\\\\n@ranchlai\\\\n@steremma\\\\n@muelletm\\\\n@lhoestq\\\\n\\\\n## Languages [...] ## Latest commit\\\\n\\\\n## History\\\\n\\\\n## Repository files navigation\\\\n\\\\n# QLoRA: Efficient Finetuning of Quantized LLMs\\\\n\\\\n| Paper | Adapter Weights | Demo |\\\\n\\\\nThis repo supports the paper \\\\\"QLoRA: Efficient Finetuning of Quantized LLMs\\\\\", an effort to democratize access to LLM research.\\\\n\\\\nQLoRA uses bitsandbytes for quantization and is integrated with Hugging Face\\'s PEFT and transformers libraries. QLoRA was developed by members of the University of Washington\\'s UW NLP group.\\\\n\\\\n## Updates\\\\n\\\\n## Overview [...] You can specify the path to your dataset using the `--dataset` argument. If the `--dataset_format` argument is not set, it will default to the Alpaca format. Here are a few examples:\\\\n\\\\n`--dataset`\\\\n`--dataset_format`\\\\n\\\\n### Multi GPU\\\\n\\\\nMulti GPU training and inference work out-of-the-box with Hugging Face\\'s Accelerate. Note that the `per_device_train_batch_size` and `per_device_eval_batch_size` arguments are global batch sizes unlike what their name suggest.\", \"score\": 0.3206303}, {\"title\": \"Fine-Tuning Llama2 with QLoRA â€” torchtune 0.6 documentation\", \"url\": \"https://docs.pytorch.org/torchtune/0.6/tutorials/qlora_finetune.html\", \"content\": \"The QLoRA authors introduce two key abstractions to decrease memory usage and avoid accuracy degradation: the bespoke 4-bit NormatFloat\\\\ntype, and a double quantization method that quantizes the quantization parameters themselves to save even more memory. torchtune uses\\\\nthe NF4Tensor abstraction from the torchao library to build QLoRA components as specified in the paper.\\\\ntorchao is a PyTorch-native library that allows you to quantize and prune your models.\\\\n\\\\n## Using QLoRA to save memoryÂ¶ [...] Catch up on the latest technical news and happenings\\\\n\\\\nStories from the PyTorch ecosystem\\\\n\\\\nLearn about the latest PyTorch tutorials, new, and more\\\\n\\\\nLearn how our community solves real, everyday machine learning problems with PyTorch\\\\n\\\\nFind events, webinars, and podcasts\\\\n\\\\nStay up-to-date with the latest updates\\\\n\\\\nLearn more about the PyTorch Foundation\\\\n\\\\nGetting Started\\\\n\\\\nFinetuning Recipes\\\\n\\\\nBasics\\\\n\\\\nTutorials\\\\n\\\\nDeep-Dives\\\\n\\\\nAPI Reference\\\\n\\\\n# Fine-Tuning Llama2 with QLoRAÂ¶ [...] QLoRA further quantizes the base model parameters into a bespoke 4-bit NormalFloat (NF4) data type, resulting in 4-8x less parameter memory usage while\\\\nlargely retaining model accuracy. As a result, the vast majority of parameters only take up 4 bits (as opposed to 16 or 32 bits by bf16/fp32 dtypes). This\\\\nquantization is done through the method highlighted in the original QLoRA paper. Adapter\", \"score\": 0.15996428}, {\"title\": \"[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - arXiv\", \"url\": \"https://arxiv.org/abs/2305.14314\", \"content\": \"close this message\\\\narXiv smileybones\\\\n\\\\n## arXiv Is Hiring a DevOps Engineer\\\\n\\\\nWork on one of the world\\'s most important websites and make an impact on open science.\\\\n\\\\nCornell University\\\\n\\\\narXiv Is Hiring a DevOps Engineer\\\\n\\\\narxiv logo\\\\n\\\\nHelp | Advanced Search\\\\n\\\\narXiv logo\\\\nCornell University Logo\\\\n\\\\n## quick links\\\\n\\\\n# Computer Science > Machine Learning\\\\n\\\\n# Title:QLoRA: Efficient Finetuning of Quantized LLMs [...] Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\\\n\\\\nHave an idea for a project that will add value for arXiv\\'s community? Learn more about arXivLabs.\\\\n\\\\narXiv Operational Status   \\\\nGet status notifications via\\\\nemail\\\\nor slack [...] # Code, Data and Media Associated with this Article\\\\n\\\\n# Demos\\\\n\\\\n# Recommenders and Search Tools\\\\n\\\\n# arXivLabs: experimental projects with community collaborators\\\\n\\\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\", \"score\": 0.1543763}]', name='tavily_search_results_json', id='5095cae6-f175-42f4-829d-5920b3baad22', tool_call_id='call_5iY1dSinISEsiUIzk3ZF2wps', artifact={'query': 'latest Tweet of the second author of QLoRA', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.scirp.org/reference/referencespapers?referenceid=3652690', 'title': 'Dettmers, T., Pagnoni, A., Holtzman, A. and Zettlemoyer, L. (2023 ...', 'content': 'Dettmers, T., Pagnoni, A., Holtzman, A. and Zettlemoyer, L. (2023) QLoRA: Efficient Finetuning of Quantized LLMs.\\n\\nhas been cited by the following article:\\n\\nTITLE:\\nSmaller & Smarter: Score-Driven Network Chaining of Smaller Language Models\\n\\nAUTHORS: \\nGunika Dhingra, Siddansh Chawla, Vijay K. Madisetti, Arshdeep Bahga [...] #### Journals by Subject\\n\\n#### Publish with us\\n\\n|  |  |  |  |\\n| --- | --- | --- | --- |\\n| Twitter | Facebook | Linkedin | Weibo |\\n\\nTwitter\\nFacebook\\nLinkedin\\nWeibo\\n\\n|  |  |\\n| --- | --- |\\n|  | customer@scirp.org |\\n| WhatsApp | +86 18163351462(WhatsApp) |\\n| Click here to send a message to me | 1655362766 |\\n|  |  |\\n|  | Paper Publishing WeChat |\\n\\nWhatsApp\\nClick here to send a message to me\\n\\n#### Article citationsMore>> [...] |  |  |  |  |\\n| --- | --- | --- | --- |\\n| Twitter | Facebook | Linkedin | Weibo |\\n\\nTwitter\\nFacebook\\nLinkedin\\nWeibo\\n\\n|  |  |\\n| --- | --- |\\n|  | customer@scirp.org |\\n| WhatsApp | +86 18163351462(WhatsApp) |\\n| Click here to send a message to me | 1655362766 |\\n|  |  |\\n|  | Paper Publishing WeChat |\\n\\nWhatsApp\\nClick here to send a message to me\\n\\n#### Home\\n\\n#### About SCIRP\\n\\n#### Service\\n\\n#### Policies', 'score': 0.51822984, 'raw_content': None}, {'url': 'https://twitter.com/search?q=%23QLORA&src=hashtag_click', 'title': '#QLORA - Search / X', 'content': '... Author: Piyush Thakur #AIforHealthcare #BrainTumorDetection #PaliGemma2 #HuggingFace #Transformers #MedicalAI #VisionLanguageModels #QLoRA #PyImageSearch.', 'score': 0.4921643, 'raw_content': None}, {'url': 'https://github.com/artidoro/qlora', 'title': 'artidoro/qlora - Efficient Finetuning of Quantized LLMs - GitHub', 'content': 'There was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n### Stars\\n\\n### Watchers\\n\\n### Forks\\n\\n## Releases\\n\\n## Packages 0\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n## Contributors 16\\n\\n@artidoro\\n@TimDettmers\\n@tobi\\n@KKcorps\\n@Birch-san\\n@pmysl\\n@bubundas17\\n@abhilash1910\\n@Qubitium\\n@dameikle\\n@ranchlai\\n@steremma\\n@muelletm\\n@lhoestq\\n\\n## Languages [...] ## Latest commit\\n\\n## History\\n\\n## Repository files navigation\\n\\n# QLoRA: Efficient Finetuning of Quantized LLMs\\n\\n| Paper | Adapter Weights | Demo |\\n\\nThis repo supports the paper \"QLoRA: Efficient Finetuning of Quantized LLMs\", an effort to democratize access to LLM research.\\n\\nQLoRA uses bitsandbytes for quantization and is integrated with Hugging Face\\'s PEFT and transformers libraries. QLoRA was developed by members of the University of Washington\\'s UW NLP group.\\n\\n## Updates\\n\\n## Overview [...] You can specify the path to your dataset using the `--dataset` argument. If the `--dataset_format` argument is not set, it will default to the Alpaca format. Here are a few examples:\\n\\n`--dataset`\\n`--dataset_format`\\n\\n### Multi GPU\\n\\nMulti GPU training and inference work out-of-the-box with Hugging Face\\'s Accelerate. Note that the `per_device_train_batch_size` and `per_device_eval_batch_size` arguments are global batch sizes unlike what their name suggest.', 'score': 0.3206303, 'raw_content': None}, {'url': 'https://docs.pytorch.org/torchtune/0.6/tutorials/qlora_finetune.html', 'title': 'Fine-Tuning Llama2 with QLoRA â€” torchtune 0.6 documentation', 'content': 'The QLoRA authors introduce two key abstractions to decrease memory usage and avoid accuracy degradation: the bespoke 4-bit NormatFloat\\ntype, and a double quantization method that quantizes the quantization parameters themselves to save even more memory. torchtune uses\\nthe NF4Tensor abstraction from the torchao library to build QLoRA components as specified in the paper.\\ntorchao is a PyTorch-native library that allows you to quantize and prune your models.\\n\\n## Using QLoRA to save memoryÂ¶ [...] Catch up on the latest technical news and happenings\\n\\nStories from the PyTorch ecosystem\\n\\nLearn about the latest PyTorch tutorials, new, and more\\n\\nLearn how our community solves real, everyday machine learning problems with PyTorch\\n\\nFind events, webinars, and podcasts\\n\\nStay up-to-date with the latest updates\\n\\nLearn more about the PyTorch Foundation\\n\\nGetting Started\\n\\nFinetuning Recipes\\n\\nBasics\\n\\nTutorials\\n\\nDeep-Dives\\n\\nAPI Reference\\n\\n# Fine-Tuning Llama2 with QLoRAÂ¶ [...] QLoRA further quantizes the base model parameters into a bespoke 4-bit NormalFloat (NF4) data type, resulting in 4-8x less parameter memory usage while\\nlargely retaining model accuracy. As a result, the vast majority of parameters only take up 4 bits (as opposed to 16 or 32 bits by bf16/fp32 dtypes). This\\nquantization is done through the method highlighted in the original QLoRA paper. Adapter', 'score': 0.15996428, 'raw_content': None}, {'url': 'https://arxiv.org/abs/2305.14314', 'title': '[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - arXiv', 'content': \"close this message\\narXiv smileybones\\n\\n## arXiv Is Hiring a DevOps Engineer\\n\\nWork on one of the world's most important websites and make an impact on open science.\\n\\nCornell University\\n\\narXiv Is Hiring a DevOps Engineer\\n\\narxiv logo\\n\\nHelp | Advanced Search\\n\\narXiv logo\\nCornell University Logo\\n\\n## quick links\\n\\n# Computer Science > Machine Learning\\n\\n# Title:QLoRA: Efficient Finetuning of Quantized LLMs [...] Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\\n\\narXiv Operational Status   \\nGet status notifications via\\nemail\\nor slack [...] # Code, Data and Media Associated with this Article\\n\\n# Demos\\n\\n# Recommenders and Search Tools\\n\\n# arXivLabs: experimental projects with community collaborators\\n\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\", 'score': 0.1543763, 'raw_content': None}], 'response_time': 4.79}), ToolMessage(content='[{\"title\": \"QLoRA: A new way to finetune LLMs | by Darius Singh | Medium\", \"url\": \"https://medium.com/@dariussingh/qlora-a-new-way-to-finetune-llms-4a5ff292903d\", \"content\": \"Sign up\\\\n\\\\nSign in\\\\n\\\\nSign up\\\\n\\\\nSign in\\\\n\\\\n# QLoRA: A new way to finetune LLMs\\\\n\\\\nDarius Singh\\\\n\\\\n--\\\\n\\\\nListen\\\\n\\\\nShare\\\\n\\\\nQLoRA: Efficient Finetuning of Quantized LLMs by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman and Luke Zettlemoyer. [...] 3) Paged Optimizers:\\\\n\\\\n# Key Insights\\\\n\\\\nKey insights from experiments:\\\\n\\\\nDid I miss anything? What are some LLM papers I should summarize next? Letâ€™s continue the conversation in the comments and learn from each other.\\\\n\\\\nPaper: \\\\n\\\\nGithub:  and \\\\n\\\\n--\\\\n\\\\n--\\\\n\\\\nDarius Singh\\\\nDarius Singh\\\\n\\\\n## Written by Darius Singh\\\\n\\\\nMachine Learning Entrepreneur\\\\n\\\\n## No responses yet\\\\n\\\\nHelp\\\\n\\\\nStatus\\\\n\\\\nAbout\\\\n\\\\nCareers\\\\n\\\\nPress\\\\n\\\\nBlog\\\\n\\\\nPrivacy\\\\n\\\\nRules\\\\n\\\\nTerms\\\\n\\\\nText to speech [...] QLoRA is an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pre-trained language model into Low-Rank Adapters (LoRA).\\\\n\\\\n# Innovations\\\\n\\\\nQLoRA introduces the following innovations to reduce memory usage without sacrificing performance:  \\\\n1) 4-bit NormalFloat (NF4):\\\\n\\\\n2) Double Quantization (DQ):\", \"score\": 0.37751824}, {\"title\": \"QLoRA 4-bit quantization #1595 - ggml-org llama.cpp - GitHub\", \"url\": \"https://github.com/ggerganov/llama.cpp/discussions/1595\", \"content\": \"There was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n# {{title}}\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n#### unbounded May 26, 2023 Author\\\\n\\\\n- [...] There was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n# {{editor}}\\'s edit\\\\n\\\\n# {{editor}}\\'s edit\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n### ikawrakow May 27, 2023\\\\n\\\\nikawrakow\\\\n\\\\n- [...] The author actually acknowledged that GPTQ quantization is superior to NF4: \\\\n\\\\nBeta\\\\nWas this translation helpful?\\\\nGive feedback.\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n# {{title}}\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n### ikawrakow May 26, 2023\\\\n\\\\n-\", \"score\": 0.32966194}, {\"title\": \"artidoro/qlora - Efficient Finetuning of Quantized LLMs - GitHub\", \"url\": \"https://github.com/artidoro/qlora\", \"content\": \"There was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n### Stars\\\\n\\\\n### Watchers\\\\n\\\\n### Forks\\\\n\\\\n## Releases\\\\n\\\\n## Packages 0\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n## Contributors 16\\\\n\\\\n@artidoro\\\\n@TimDettmers\\\\n@tobi\\\\n@KKcorps\\\\n@Birch-san\\\\n@pmysl\\\\n@bubundas17\\\\n@abhilash1910\\\\n@Qubitium\\\\n@dameikle\\\\n@ranchlai\\\\n@steremma\\\\n@muelletm\\\\n@lhoestq\\\\n\\\\n## Languages [...] ## Latest commit\\\\n\\\\n## History\\\\n\\\\n## Repository files navigation\\\\n\\\\n# QLoRA: Efficient Finetuning of Quantized LLMs\\\\n\\\\n| Paper | Adapter Weights | Demo |\\\\n\\\\nThis repo supports the paper \\\\\"QLoRA: Efficient Finetuning of Quantized LLMs\\\\\", an effort to democratize access to LLM research.\\\\n\\\\nQLoRA uses bitsandbytes for quantization and is integrated with Hugging Face\\'s PEFT and transformers libraries. QLoRA was developed by members of the University of Washington\\'s UW NLP group.\\\\n\\\\n## Updates\\\\n\\\\n## Overview [...] You can specify the path to your dataset using the `--dataset` argument. If the `--dataset_format` argument is not set, it will default to the Alpaca format. Here are a few examples:\\\\n\\\\n`--dataset`\\\\n`--dataset_format`\\\\n\\\\n### Multi GPU\\\\n\\\\nMulti GPU training and inference work out-of-the-box with Hugging Face\\'s Accelerate. Note that the `per_device_train_batch_size` and `per_device_eval_batch_size` arguments are global batch sizes unlike what their name suggest.\", \"score\": 0.31538844}, {\"title\": \"Advanced fine-tuning methods on Amazon SageMaker AI - AWS\", \"url\": \"https://aws.amazon.com/blogs/machine-learning/advanced-fine-tuning-methods-on-amazon-sagemaker-ai/\", \"content\": \"most used techniques is Quantized Low-Rank Adaptation (QLoRA).QLoRA is an efficient fine-tuning technique for LLMs that combines quantization and LoRA approaches. It uses 4-bit quantization to reduce model memory usage while maintaining model weights in 4-bit precision during training and employs double quantization for further memory reduction. The technique integrates LoRA by adding trainable rank decomposition matrices and keeping adapter parameters in 16-bit precision, enabling PEFT. QLoRA [...] | QLoRA (Quantized LoRA) | SageMaker Training (custom implementation) | Combines model quantization with LoRA, loading the base model in 4-bit precision while adapting it with trainable LoRA parameters | Further reduces memory requirements compared to standard LoRA | [...] We also address the three fundamental aspects of LLM development: the core lifecycle stages, the spectrum of fine-tuning methodologies, and the critical alignment techniques that provide responsible AI deployment. We explore how Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and QLoRA have democratized model adaptation, so organizations of all sizes can customize large models to their specific needs. Additionally, we examine alignment approaches such as Reinforcement Learning from\", \"score\": 0.20683034}, {\"title\": \"[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - arXiv\", \"url\": \"https://arxiv.org/abs/2305.14314\", \"content\": \"close this message\\\\narXiv smileybones\\\\n\\\\n## arXiv Is Hiring a DevOps Engineer\\\\n\\\\nWork on one of the world\\'s most important websites and make an impact on open science.\\\\n\\\\nCornell University\\\\n\\\\narXiv Is Hiring a DevOps Engineer\\\\n\\\\narxiv logo\\\\n\\\\nHelp | Advanced Search\\\\n\\\\narXiv logo\\\\nCornell University Logo\\\\n\\\\n## quick links\\\\n\\\\n# Computer Science > Machine Learning\\\\n\\\\n# Title:QLoRA: Efficient Finetuning of Quantized LLMs [...] Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\\\n\\\\nHave an idea for a project that will add value for arXiv\\'s community? Learn more about arXivLabs.\\\\n\\\\narXiv Operational Status   \\\\nGet status notifications via\\\\nemail\\\\nor slack [...] # Code, Data and Media Associated with this Article\\\\n\\\\n# Demos\\\\n\\\\n# Recommenders and Search Tools\\\\n\\\\n# arXivLabs: experimental projects with community collaborators\\\\n\\\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\", \"score\": 0.16091137}]', name='tavily_search_results_json', id='cca5228b-42d6-409b-8fca-ae15c48ee85a', tool_call_id='call_0eeIMsjmWOK47kTneIiujqfs', artifact={'query': 'latest Tweet of the third author of QLoRA', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://medium.com/@dariussingh/qlora-a-new-way-to-finetune-llms-4a5ff292903d', 'title': 'QLoRA: A new way to finetune LLMs | by Darius Singh | Medium', 'content': 'Sign up\\n\\nSign in\\n\\nSign up\\n\\nSign in\\n\\n# QLoRA: A new way to finetune LLMs\\n\\nDarius Singh\\n\\n--\\n\\nListen\\n\\nShare\\n\\nQLoRA: Efficient Finetuning of Quantized LLMs by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman and Luke Zettlemoyer. [...] 3) Paged Optimizers:\\n\\n# Key Insights\\n\\nKey insights from experiments:\\n\\nDid I miss anything? What are some LLM papers I should summarize next? Letâ€™s continue the conversation in the comments and learn from each other.\\n\\nPaper: \\n\\nGithub:  and \\n\\n--\\n\\n--\\n\\nDarius Singh\\nDarius Singh\\n\\n## Written by Darius Singh\\n\\nMachine Learning Entrepreneur\\n\\n## No responses yet\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nRules\\n\\nTerms\\n\\nText to speech [...] QLoRA is an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pre-trained language model into Low-Rank Adapters (LoRA).\\n\\n# Innovations\\n\\nQLoRA introduces the following innovations to reduce memory usage without sacrificing performance:  \\n1) 4-bit NormalFloat (NF4):\\n\\n2) Double Quantization (DQ):', 'score': 0.37751824, 'raw_content': None}, {'url': 'https://github.com/ggerganov/llama.cpp/discussions/1595', 'title': 'QLoRA 4-bit quantization #1595 - ggml-org llama.cpp - GitHub', 'content': \"There was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n# {{title}}\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n#### unbounded May 26, 2023 Author\\n\\n- [...] There was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n# {{editor}}'s edit\\n\\n# {{editor}}'s edit\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n### ikawrakow May 27, 2023\\n\\nikawrakow\\n\\n- [...] The author actually acknowledged that GPTQ quantization is superior to NF4: \\n\\nBeta\\nWas this translation helpful?\\nGive feedback.\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n# {{title}}\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n### ikawrakow May 26, 2023\\n\\n-\", 'score': 0.32966194, 'raw_content': None}, {'url': 'https://github.com/artidoro/qlora', 'title': 'artidoro/qlora - Efficient Finetuning of Quantized LLMs - GitHub', 'content': 'There was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n### Stars\\n\\n### Watchers\\n\\n### Forks\\n\\n## Releases\\n\\n## Packages 0\\n\\n### Uh oh!\\n\\nThere was an error while loading. Please reload this page.\\n\\nThere was an error while loading. Please reload this page.\\n\\n## Contributors 16\\n\\n@artidoro\\n@TimDettmers\\n@tobi\\n@KKcorps\\n@Birch-san\\n@pmysl\\n@bubundas17\\n@abhilash1910\\n@Qubitium\\n@dameikle\\n@ranchlai\\n@steremma\\n@muelletm\\n@lhoestq\\n\\n## Languages [...] ## Latest commit\\n\\n## History\\n\\n## Repository files navigation\\n\\n# QLoRA: Efficient Finetuning of Quantized LLMs\\n\\n| Paper | Adapter Weights | Demo |\\n\\nThis repo supports the paper \"QLoRA: Efficient Finetuning of Quantized LLMs\", an effort to democratize access to LLM research.\\n\\nQLoRA uses bitsandbytes for quantization and is integrated with Hugging Face\\'s PEFT and transformers libraries. QLoRA was developed by members of the University of Washington\\'s UW NLP group.\\n\\n## Updates\\n\\n## Overview [...] You can specify the path to your dataset using the `--dataset` argument. If the `--dataset_format` argument is not set, it will default to the Alpaca format. Here are a few examples:\\n\\n`--dataset`\\n`--dataset_format`\\n\\n### Multi GPU\\n\\nMulti GPU training and inference work out-of-the-box with Hugging Face\\'s Accelerate. Note that the `per_device_train_batch_size` and `per_device_eval_batch_size` arguments are global batch sizes unlike what their name suggest.', 'score': 0.31538844, 'raw_content': None}, {'url': 'https://aws.amazon.com/blogs/machine-learning/advanced-fine-tuning-methods-on-amazon-sagemaker-ai/', 'title': 'Advanced fine-tuning methods on Amazon SageMaker AI - AWS', 'content': 'most used techniques is Quantized Low-Rank Adaptation (QLoRA).QLoRA is an efficient fine-tuning technique for LLMs that combines quantization and LoRA approaches. It uses 4-bit quantization to reduce model memory usage while maintaining model weights in 4-bit precision during training and employs double quantization for further memory reduction. The technique integrates LoRA by adding trainable rank decomposition matrices and keeping adapter parameters in 16-bit precision, enabling PEFT. QLoRA [...] | QLoRA (Quantized LoRA) | SageMaker Training (custom implementation) | Combines model quantization with LoRA, loading the base model in 4-bit precision while adapting it with trainable LoRA parameters | Further reduces memory requirements compared to standard LoRA | [...] We also address the three fundamental aspects of LLM development: the core lifecycle stages, the spectrum of fine-tuning methodologies, and the critical alignment techniques that provide responsible AI deployment. We explore how Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and QLoRA have democratized model adaptation, so organizations of all sizes can customize large models to their specific needs. Additionally, we examine alignment approaches such as Reinforcement Learning from', 'score': 0.20683034, 'raw_content': None}, {'url': 'https://arxiv.org/abs/2305.14314', 'title': '[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs - arXiv', 'content': \"close this message\\narXiv smileybones\\n\\n## arXiv Is Hiring a DevOps Engineer\\n\\nWork on one of the world's most important websites and make an impact on open science.\\n\\nCornell University\\n\\narXiv Is Hiring a DevOps Engineer\\n\\narxiv logo\\n\\nHelp | Advanced Search\\n\\narXiv logo\\nCornell University Logo\\n\\n## quick links\\n\\n# Computer Science > Machine Learning\\n\\n# Title:QLoRA: Efficient Finetuning of Quantized LLMs [...] Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\\n\\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\\n\\narXiv Operational Status   \\nGet status notifications via\\nemail\\nor slack [...] # Code, Data and Media Associated with this Article\\n\\n# Demos\\n\\n# Recommenders and Search Tools\\n\\n# arXivLabs: experimental projects with community collaborators\\n\\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\", 'score': 0.16091137, 'raw_content': None}], 'response_time': 3.78})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='I found the QLoRA paper on arXiv, titled \"QLoRA: Efficient Finetuning of Quantized LLMs,\" authored by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. \\n\\nI also searched for the latest Tweets from each of these authors:\\n- Tim Dettmers\\n- Artidoro Pagnoni\\n- Ari Holtzman\\n- Luke Zettlemoyer\\n\\nWould you like to see the specific Tweets or any additional information?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 6707, 'total_tokens': 6817, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BtTOl6M5UU1hchhT13hSHoEYmLRlo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ff46b66f-edbd-4708-a3d7-fddde2dd2ccd-0', usage_metadata={'input_tokens': 6707, 'output_tokens': 110, 'total_tokens': 6817, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using Tavily!\")]}\n",
        "\n",
        "async for chunk in simple_agent_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "---\n",
        "\n",
        "At first, we load our task/content using the message state.\n",
        "The message state is passed through the start node of our graph, that is the agent node.\n",
        "The agent node sees we have tool_calls available called the 'arxiv'. It thinks that we can use this to get info regarding the QLoRA paper to get the context.\n",
        "\n",
        "Now, we receive response from the action node. The LLM used the tool arxiv and got few context out of the QLoRA and fetched the tweets of the author.\n",
        "\n",
        "The context is read by agent and it finds the QLoRA paper on arXiv and author tweets of the paper with the title \"QLoRA: Efficient Finetuning of Quantized LLM\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤ Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain_with_formatting = convert_inputs | simple_agent_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "76be837b-6424-4516-8f63-07fbd8c25bf5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG, or Retrieval-Augmented Generation, is an AI framework that enhances the capabilities of large language models (LLMs) by integrating an external information retrieval process. This process involves searching for and retrieving relevant information from external data sources, such as databases, documents, or web sources, before generating a response. The main goal of RAG is to produce more accurate, relevant, and up-to-date responses by grounding the model's output in external knowledge, rather than relying solely on the information contained within the model's training data. This approach helps reduce inaccuracies and ensures that the generated content is aligned with the latest available information.\""
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain_with_formatting.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "\n",
        "questions = [\n",
        "    \"What problem does FlashAttention solve?\",\n",
        "    \"What type of memory optimization is used in FlashAttention?\",\n",
        "    \"Who developed the FlashAttention algorithm?\",\n",
        "    \"How does FlashAttention differ from standard attention?\",\n",
        "    \"Why is FlashAttention useful for large models?\",\n",
        "    \"What are the main components of FlashAttention 2?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\": [\"memory\", \"bottleneck\"]},\n",
        "    {\"must_mention\": [\"tiling\", \"SRAM\"]},\n",
        "    {\"must_mention\": [\"Tri\", \"Dao\"]},\n",
        "    {\"must_mention\": [\"faster\", \"exact\"]},\n",
        "    {\"must_mention\": [\"scale\", \"efficient\"]},\n",
        "    {\"must_mention\": [\"kernel\", \"bandwidth\"]},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'example_ids': ['89091e3f-a74d-4348-9e58-7648ddc33847',\n",
              "  '30ab38a2-edbd-418d-bbc6-05d3a26a8386',\n",
              "  '55411111-f9b7-4632-95e5-f6b3b663dbf2',\n",
              "  'f9ad6f35-637d-43c9-ad88-6ac1bac69d51',\n",
              "  'ed31fd48-7b3d-41ff-a523-941aad090408',\n",
              "  '03d9da6b-0ad0-41a9-8fa7-dcd8aedfc788'],\n",
              " 'count': 6}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### â“ Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "---\n",
        "The correct answers are associated with the questions by their index in the list. Each question in the questions list has a corresponding dictionary in the answers list at the same position. This dictionary contains:\n",
        "\n",
        "A \"must_mention\" list: words or phrases that must be present in the generated answer for it to be considered correct.\n",
        "\n",
        "Optionally, a \"sample_answer\": a reference answer used for comparison or reporting.\n",
        "\n",
        "For example:\n",
        "\n",
        "```python\n",
        "questions[2] = \"Who developed the FlashAttention algorithm?\"\n",
        "answers[2] = {\n",
        "    \"must_mention\": [\"Tri\", \"Dao\"],\n",
        "    \"sample_answer\": \"The FlashAttention algorithm was developed by Tri Dao and his collaborators at Stanford University.\"\n",
        "}\n",
        "```\n",
        "This means that:\n",
        "\n",
        "The correct answer to Question #3 is expected to mention both \"Tri\" and \"Dao\" to pass an automated validation check.\n",
        "\n",
        "The association is purely positional â€” the i-th question maps to the i-th answer.\n",
        "\n",
        "This setup allows easy batch evaluation by looping through zip(questions, answers) in Python.\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### â“ Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "For our case, only one evaluation failed. Looking at the reason why it failed:\n",
        "\n",
        "The must_mention list was:\n",
        "\n",
        "```json\n",
        "[\"kernel\", \"bandwidth\"]\n",
        "```\n",
        "But the model's output did not include the word \"kernel\" â€” it said:\n",
        "\n",
        "**\"...based on NVIDIA's CUTLASS Library... for efficient matrix multiplication and memory operations.\"**\n",
        "\n",
        "It described kernels, but never literally said the word \"kernel\". So:\n",
        "\n",
        "```python\n",
        "all(phrase in prediction for phrase in required)  # => False\n",
        "```\n",
        "Thus, it was marked incorrect.\n",
        "\n",
        "---\n",
        "\n",
        "So, to fix the issue, we can optimize our evaluator:\n",
        "\n",
        "instead of `all(phrase in prediction for phrase in required)`, where we are hard checking the exact term such as ***kernel***, we need to do a semantic search with something like this: `util.cos_sim(model.encode(phrase), model.encode(text))[0][0] > 0.6`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "efcf57067cf743d8b4ce059a61cbe02e",
            "53e33aae3b97490c82aec7bbb0d6ebba",
            "ad84e0e971d3455db2efe7dd0d1f803e",
            "72adef9b70dd48198b7322b6c5b113cf",
            "8a61d045ffd44ac58f3f13eb10044836",
            "041e22a9b5514e36bd4d1dac01d5d398",
            "886d762f2a7c421382efb5502c6d42a1",
            "ab91fd625bbd43afbf8c6398193a88d0",
            "716557ad09874dcb989d75f7c74424cd",
            "77d4c0ebaae045b58efc4f789c9a2360",
            "0d622ccc56264fac8fd7508dbdbe6e29"
          ]
        },
        "id": "p5TeCUUkuGld",
        "outputId": "2f7d62a2-e78d-447a-d07b-f9e4d500fb79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'Search Pipeline - Evaluation - 5c74-26c28a09' at:\n",
            "https://smith.langchain.com/o/d271d21b-9aba-41e9-ab3b-0c1eb6900d84/datasets/87f65c83-4760-4126-a182-7264f1d85246/compare?selectedSessions=102485dd-0bdd-4007-8bc2-45092ad40ba7\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d006b4ed05e0480985739c7891ce95f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "experiment_results = client.evaluate(\n",
        "    agent_chain_with_formatting,\n",
        "    data=dataset_name,\n",
        "    evaluators=[must_mention],\n",
        "    experiment_prefix=f\"Search Pipeline - Evaluation - {uuid4().hex[0:4]}\",\n",
        "    metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image info](evaluation.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "eeEqU7s05Byu",
        "outputId": "78395075-a05d-4ebd-c798-ed968b935318"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<ExperimentResults Search Pipeline - Evaluation - 5c74-26c28a09>"
            ],
            "text/plain": [
              "<ExperimentResults Search Pipeline - Evaluation - 5c74-26c28a09>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "experiment_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "This cell creates a new state graph named graph_with_helpfulness_check with an agent state, then adds two nodes:\n",
        "\n",
        "\"agent\" node that calls the language model (call_model)\n",
        "\n",
        "\"action\" node that runs a tool (tool_node)\n",
        "\n",
        "Essentially, it sets up a workflow where the agent can generate responses and perform tool actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r6XXA5FJbVf",
        "outputId": "ff713041-e498-4f0f-a875-a03502b87729"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11e45e990>"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "This line sets the starting point of the graph to the \"agent\" node â€” meaning the graphâ€™s execution begins with the agent generating or processing the input first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNWHwWxuRiLY",
        "outputId": "295f5a35-ceff-452a-ffb8-c52eada6a816"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11e45e990>"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "This function tool_call_or_helpful does the following:\n",
        "\n",
        "Checks if the last message in the conversation contains any tool calls. If yes, it directs to \"action\" (meaning the agent should invoke a tool).\n",
        "\n",
        "If there are more than 10 messages in the conversation, it stops by returning \"END\".\n",
        "\n",
        "Otherwise, it constructs a prompt asking a GPT-4.1-mini model whether the final response is â€œextremely helpfulâ€ based on the initial query and final response.\n",
        "\n",
        "The model returns \"Y\" for helpful or \"N\" for unhelpful.\n",
        "\n",
        "If helpful, the function returns \"end\" to finish; if not, it returns \"continue\" to keep going."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  helpfullness_prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "\n",
        "  helpfulness_chain = helpfullness_prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "#### ðŸ—ï¸ Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!\n",
        "\n",
        "The tool_call_or_helpful function guides the agentâ€™s next action by first checking if the last message includes any tool calls; if so, it routes to execute those tools. If the conversation exceeds ten messages, it ends the interaction. Otherwise, it prompts an LLM to evaluate whether the final response is extremely helpful based on the initial query. Depending on the LLMâ€™s answerâ€”â€œYâ€ for helpful or â€œNâ€ for notâ€”it either ends the conversation or continues refining the response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "This code sets up the decision flow for the \"agent\" node in the graph_with_helpfulness_check by using the tool_call_or_helpful function to determine the next step. Depending on the functionâ€™s output, the graph either loops back to the \"agent\" node to continue processing, moves to the \"action\" node to execute a tool call, or ends the workflow if the function signals completion. This allows the graph to dynamically control the agentâ€™s behavior based on the conversation state and helpfulness evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVTKnWMbP_8T",
        "outputId": "7f729b1f-311c-4084-ceaf-0da437900c85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11e45e990>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "This line connects the \"action\" node back to the \"agent\" node in the graph, enabling the workflow to return to the agent after a tool action is performed. This allows the agent to process the results of the tool call and continue the conversation or take further actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbDK2MbuREgU",
        "outputId": "21a64c20-27a1-4e0e-afde-a639abaa8b55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x11e45e990>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "This line compiles the graph_with_helpfulness_check into a runnable agent called agent_with_helpfulness_check, preparing the graphâ€™s nodes and edges for execution. After this, you can use the compiled agent to process inputs and follow the defined workflow with helpfulness checks and tool calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "This code runs the compiled agent agent_with_helpfulness_check asynchronously on an input message asking about LoRA, Tim Dettmers, and Attention. It streams partial results (stream_mode=\"updates\") from the agentâ€™s nodes, printing updates as they arrive. For each chunk of output, it shows which node produced the message and the message content, allowing you to observe the agentâ€™s step-by-step reasoning, tool calls, and final answers in real time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "f152dea8-96ad-4d29-d8b2-a064c96a8bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_rD772m75ZviFxikjzoe21CMj', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_PHRUAJvjHyI9bPdAMeAmT4t5', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}, {'id': 'call_8nllw3FyCKCRMjuDsBtwabTw', 'function': {'arguments': '{\"query\": \"Attention in machine learning\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 79, 'prompt_tokens': 203, 'total_tokens': 282, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BtU1pa3XTGEMSmRmbyBUFxlCP9bR5', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--836d012b-e8f9-45a0-8534-5940d6600d47-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_rD772m75ZviFxikjzoe21CMj', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_PHRUAJvjHyI9bPdAMeAmT4t5', 'type': 'tool_call'}, {'name': 'tavily_search_results_json', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_8nllw3FyCKCRMjuDsBtwabTw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 203, 'output_tokens': 79, 'total_tokens': 282, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='[{\"title\": \"What is LoRA? | Low-rank adaptation - Cloudflare\", \"url\": \"https://www.cloudflare.com/learning/ai/what-is-lora/\", \"content\": \"Low-rank adaptation (LoRA) is a technique for quickly adapting machine learning models to new contexts. LoRA helps make huge and complicated machine learning models much more suited for specific uses. It works by adding lightweight pieces to the original model, as opposed to changing the entire model. LoRA helps developers quickly expand the use cases for the machine learning models they build.\\\\n\\\\n## What does LoRA do? [...] LoRA adds low-rank matrices to the frozen original machine learning model. These matrices contain new weights to apply to the model when generating results. This process alters the outputs that the model produces with minimal computing power and training time.\\\\n\\\\nIn the analogy used above, Jim bought cheap adapters to plug his appliances into the wall. Low-rank matrices are like those cheap adapters, with the outlets being the original machine learning models.\\\\n\\\\n## How does machine learning work? [...] Sign upSales: +1 (888) 99 FLARE\\\\n\\\\n# What is low-rank adaptation (LoRA)?\\\\n\\\\nLow-rank adaptation (LoRA) is a way to adapt a large machine learning model for specific uses without retraining the entire model.\\\\n\\\\n#### Learning Objectives\\\\n\\\\nAfter reading this article you will be able to:\\\\n\\\\n Define \\\\\"low-rank adaptation\\\\\" (LoRA)\\\\n Explain in simple fashion how LoRA works\\\\n Understand the advantages of using LoRA\\\\n\\\\nRelated Content\\\\n\\\\n---\", \"score\": 0.9255605}, {\"title\": \"Low-Rank Adaptation of Large Language Models (LoRA)\", \"url\": \"https://huggingface.co/docs/diffusers/v0.23.1/training/lora\", \"content\": \"Low-Rank Adaptation of Large Language Models (LoRA) is a training method that accelerates the training of large models while consuming less memory. It adds pairs of rank-decomposition weight matrices (called update matrices) to existing weights, and only trains those newly added weights. This has a couple of advantages:\", \"score\": 0.9006087}, {\"title\": \"Understanding LoRA with a minimal example - Posit AI Blog\", \"url\": \"https://blogs.rstudio.com/tensorflow/posts/2023-06-22-understanding-lora/\", \"content\": \"LoRA (Low-Rank Adaptation) is a new technique for fine tuning large scale pre-trained\\\\nmodels. Such models are usually trained on general domain data, so as to have\\\\nthe maximum amount of data. In order to obtain better results in tasks like chatting\\\\nor question answering, these models can be further â€˜fine-tunedâ€™ or adapted on domain\\\\nspecific data. [...] LoRA (Low Rank Adaptation) is a new technique for fine-tuning deep learning models that works by reducing the number of trainable parameters and enables efficient task switching. In this blog post we will talk about the key ideas behind LoRA in a very minimal torch example.\\\\n\\\\nContents [...] Understanding LoRA with a minimal example\\\\n\\\\nLoRA (Low Rank Adaptation) is a new technique for fine-tuning deep learning models that works by reducing the number of trainable parameters and enables efficient task switching. In this blog post we will talk about the key ideas behind LoRA in a very minimal torch example.\", \"score\": 0.88420963}, {\"title\": \"What is LoRA (Low-Rank Adaption)? - IBM\", \"url\": \"https://www.ibm.com/think/topics/lora\", \"content\": \"LoRA adds low-rank matrices to the frozen original machine learning model. The low-rank matrices are updated through gradient descent during fine-tuning, without modifying the weights of the base model. These matrices contain new weights to apply to the model when generating results. The multiplied change matrix is added to the base model weights to get the final fine-tuned model. This process alters the outputs that the model produces with minimal computing power and training time. [...] LoRA leverages the concept of lower-rank matrices to make the model training process extremely efficient and fast. Traditionally fine-tuning LLMs requires adjusting the entire model. LoRA focuses on modifying a smaller subset of parameters (lower-rank matrices) to reduce computational and memory overhead. [...] LoRA makes training more efficient and lowers the hardware barrier to entry because users do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, the process requires optimizing only the much smaller low-rank matrices.\\\\n\\\\nThe linear design of LoRA allows data scientists to merge the trainable matrices with the frozen pretrained model weights when deployed, introducing no inference latency compared to a fully fine-tuned model by construction.\", \"score\": 0.8810534}, {\"title\": \"LoRA can turn AI models into specialists quickly - IBM Research\", \"url\": \"https://research.ibm.com/blog/LoRAs-explained\", \"content\": \"Low-rank adaptation (LoRA) is a faster, cheaper way of turning LLMs and other foundation models into specialists. IBM Research is innovating with LoRAs to make AI models easier to customize and serve at scale.\\\\n\\\\nImage 2Image 3 [...] Low-rank adaptation (LoRA) is a quicker solution. With LoRA, you fine-tune a small subset of the base modelâ€™s weights, creating a plug-in module that gives the model expertise in, for example, biology or mathematical reasoning at inference time. Colloquially, this module is also called a â€œLoRA.â€ Like custom bits for a multi-head screwdriver, LoRAs can be swapped in and out of the base model to give it specialized capabilities. [...] The LoRA approach also makes it easier to add new skills and knowledge without overwriting what the model previously learned, a phenomenon known as catastrophic forgetting. LoRA offers a way to inject new information into a model without sacrificing performance.\", \"score\": 0.8338803}]', name='tavily_search_results_json', id='be406913-0b1d-458f-ac99-12d15fe8c038', tool_call_id='call_rD772m75ZviFxikjzoe21CMj', artifact={'query': 'LoRA machine learning', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.cloudflare.com/learning/ai/what-is-lora/', 'title': 'What is LoRA? | Low-rank adaptation - Cloudflare', 'content': 'Low-rank adaptation (LoRA) is a technique for quickly adapting machine learning models to new contexts. LoRA helps make huge and complicated machine learning models much more suited for specific uses. It works by adding lightweight pieces to the original model, as opposed to changing the entire model. LoRA helps developers quickly expand the use cases for the machine learning models they build.\\n\\n## What does LoRA do? [...] LoRA adds low-rank matrices to the frozen original machine learning model. These matrices contain new weights to apply to the model when generating results. This process alters the outputs that the model produces with minimal computing power and training time.\\n\\nIn the analogy used above, Jim bought cheap adapters to plug his appliances into the wall. Low-rank matrices are like those cheap adapters, with the outlets being the original machine learning models.\\n\\n## How does machine learning work? [...] Sign upSales: +1 (888) 99 FLARE\\n\\n# What is low-rank adaptation (LoRA)?\\n\\nLow-rank adaptation (LoRA) is a way to adapt a large machine learning model for specific uses without retraining the entire model.\\n\\n#### Learning Objectives\\n\\nAfter reading this article you will be able to:\\n\\n Define \"low-rank adaptation\" (LoRA)\\n Explain in simple fashion how LoRA works\\n Understand the advantages of using LoRA\\n\\nRelated Content\\n\\n---', 'score': 0.9255605, 'raw_content': None}, {'url': 'https://huggingface.co/docs/diffusers/v0.23.1/training/lora', 'title': 'Low-Rank Adaptation of Large Language Models (LoRA)', 'content': 'Low-Rank Adaptation of Large Language Models (LoRA) is a training method that accelerates the training of large models while consuming less memory. It adds pairs of rank-decomposition weight matrices (called update matrices) to existing weights, and only trains those newly added weights. This has a couple of advantages:', 'score': 0.9006087, 'raw_content': None}, {'url': 'https://blogs.rstudio.com/tensorflow/posts/2023-06-22-understanding-lora/', 'title': 'Understanding LoRA with a minimal example - Posit AI Blog', 'content': 'LoRA (Low-Rank Adaptation) is a new technique for fine tuning large scale pre-trained\\nmodels. Such models are usually trained on general domain data, so as to have\\nthe maximum amount of data. In order to obtain better results in tasks like chatting\\nor question answering, these models can be further â€˜fine-tunedâ€™ or adapted on domain\\nspecific data. [...] LoRA (Low Rank Adaptation) is a new technique for fine-tuning deep learning models that works by reducing the number of trainable parameters and enables efficient task switching. In this blog post we will talk about the key ideas behind LoRA in a very minimal torch example.\\n\\nContents [...] Understanding LoRA with a minimal example\\n\\nLoRA (Low Rank Adaptation) is a new technique for fine-tuning deep learning models that works by reducing the number of trainable parameters and enables efficient task switching. In this blog post we will talk about the key ideas behind LoRA in a very minimal torch example.', 'score': 0.88420963, 'raw_content': None}, {'url': 'https://www.ibm.com/think/topics/lora', 'title': 'What is LoRA (Low-Rank Adaption)? - IBM', 'content': 'LoRA adds low-rank matrices to the frozen original machine learning model. The low-rank matrices are updated through gradient descent during fine-tuning, without modifying the weights of the base model. These matrices contain new weights to apply to the model when generating results. The multiplied change matrix is added to the base model weights to get the final fine-tuned model. This process alters the outputs that the model produces with minimal computing power and training time. [...] LoRA leverages the concept of lower-rank matrices to make the model training process extremely efficient and fast. Traditionally fine-tuning LLMs requires adjusting the entire model. LoRA focuses on modifying a smaller subset of parameters (lower-rank matrices) to reduce computational and memory overhead. [...] LoRA makes training more efficient and lowers the hardware barrier to entry because users do not need to calculate the gradients or maintain the optimizer states for most parameters. Instead, the process requires optimizing only the much smaller low-rank matrices.\\n\\nThe linear design of LoRA allows data scientists to merge the trainable matrices with the frozen pretrained model weights when deployed, introducing no inference latency compared to a fully fine-tuned model by construction.', 'score': 0.8810534, 'raw_content': None}, {'url': 'https://research.ibm.com/blog/LoRAs-explained', 'title': 'LoRA can turn AI models into specialists quickly - IBM Research', 'content': 'Low-rank adaptation (LoRA) is a faster, cheaper way of turning LLMs and other foundation models into specialists. IBM Research is innovating with LoRAs to make AI models easier to customize and serve at scale.\\n\\nImage 2Image 3 [...] Low-rank adaptation (LoRA) is a quicker solution. With LoRA, you fine-tune a small subset of the base modelâ€™s weights, creating a plug-in module that gives the model expertise in, for example, biology or mathematical reasoning at inference time. Colloquially, this module is also called a â€œLoRA.â€ Like custom bits for a multi-head screwdriver, LoRAs can be swapped in and out of the base model to give it specialized capabilities. [...] The LoRA approach also makes it easier to add new skills and knowledge without overwriting what the model previously learned, a phenomenon known as catastrophic forgetting. LoRA offers a way to inject new information into a model without sacrificing performance.', 'score': 0.8338803, 'raw_content': None}], 'response_time': 1.92}), ToolMessage(content='[{\"title\": \"Tim Dettmers - AI2050 - Schmidt Sciences\", \"url\": \"https://ai2050.schmidtsciences.org/fellow/tim-dettmers/\", \"content\": \"Tim Dettmers is an Assistant Professor at Carnegie Mellon University and a Research Scientist at the Allen Institute for AI, and his research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. He has won oral, spotlight, and best paper awards at conferences [...] such as ICLR and NeurIPS. He created the bitsandbytes library for efficient deep learning, which is growing at 2.2 million installations per month, and received Google Open Source and PyTorch Foundation awards. [...] AI models like ChatGPT work well for general use but fail in specialized expert domains, such as the medical sciences. To make AI models work in expert domains, one must adapt them, which is costly and requires significant AI expertise. This project overcomes these cost and expertise barriers through two new approaches: (1) use AI models themselves to perform the AI model adaptation process automatically; (2) make the adaptation process cheap so it can be run on regular consumer hardware. With\", \"score\": 0.9245858}, {\"title\": \"CSE Faculty Candidate Seminar - Tim Dettmers\", \"url\": \"https://cse.gatech.edu/events/2024/02/20/cse-faculty-candidate-seminar-tim-dettmers\", \"content\": \"Bio:Tim Dettmersâ€™s research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. These methods enable many more people to use, adapt, or train foundation models without affecting the quality of AI predictions or generations. He is a PhD candidate at the [...] News and Events\\\\n       News\\\\n       Upcoming Events\\\\n       Calendar\\\\n       CSE Biweekly Roundup\\\\n       Analyzer\\\\n       2024 Annual Brief.pdf)\\\\n\\\\n\\\\n\\\\nSearch\\\\n------\\\\n\\\\nSearch \\\\n\\\\nBreadcrumb\\\\n----------\\\\n\\\\n1.   Home\\\\n2.   Events\\\\n\\\\nUpcoming Events\\\\n---------------\\\\n\\\\nCSE Faculty Candidate Seminar - Tim Dettmers\\\\n============================================\\\\n\\\\nImage 2: Tim Dettmers.png\\\\n\\\\nName:Tim Dettmers, Ph.D. student at University of Washington\\\\n\\\\nDate:Tuesday, February 20, 2024 at 11:00 am [...] University of Washington and has won oral, spotlight, and best paper awards at conferences such as ICLR and NeurIPS. He created the bitsandbytes library for efficient deep learning, which is growing at 1.4 million installations per month and received Google Open Source and PyTorch Foundation awards.\", \"score\": 0.70531887}, {\"title\": \"About Me - Tim Dettmers\", \"url\": \"https://timdettmers.com/about/\", \"content\": \"Research Interests  \\\\nPublications  \\\\nAwards & Honors  \\\\nService\\\\n\\\\nGoogle Scholar\\\\n\\\\nfirstname.lastname@gmail.com\\\\n\\\\nImage 1I am a research scientist at the Allen Institute for Artificial Intelligence (Ai2) and an incoming Assistant Professor at Carnegie Mellon University (CMU). I am the creator and maintainer of bitsandbytes. [...] I have a PhD from University of Washington advised by Luke Zettlemoyer working on efficient deep learning at the intersection between machine learning, natural language processing, and computer systems with a focus on quantization and sparsity. My main research goal is to empower everyone to make AI their own. I do this by making large models accessible through my research (QLoRA, LLM.int8(), k-bit inference scaling laws, Petals, SWARM) and by developing software that makes it easy to use my [...] About Me â€” Tim Dettmers\\\\n===============  \\\\n\\\\nSkip links\\\\n----------\\\\n\\\\n   Skip to primary navigation\\\\n   Skip to content\\\\n   Skip to primary sidebar\\\\n\\\\nTim Dettmers\\\\n\\\\nMaking deep learning accessible.\\\\n\\\\nHeader Right\\\\n------------\\\\n\\\\n### Blog Posts Topics\\\\n\\\\n   Academia (4)\\\\n       PhD Life (3)\\\\n   Deep Learning (7)\\\\n   Hardware (8)\\\\n   Science (4)\\\\n       Neuroscience (1)\\\\n\\\\nMain navigation\\\\n---------------\\\\n\\\\n   Blog\\\\n       Deep Learning\\\\n       Hardware\\\\n       Neuroscience\\\\n   Publications\\\\n   About Me\\\\n\\\\nAbout Me\\\\n========\", \"score\": 0.68318754}, {\"title\": \"Tim Dettmers | Carnegie Mellon University Computer Science ...\", \"url\": \"https://csd.cmu.edu/people/faculty/tim-dettmers\", \"content\": \"X\\\\n\\\\nBreadcrumb\\\\n----------\\\\n\\\\n1.  Home\\\\n2.  People\\\\n3.  Faculty\\\\n4.  Tim Dettmers\\\\n\\\\nTim Dettmers\\\\n============\\\\n\\\\nImage 2: Tim DettmersAssistant Professor\\\\n\\\\nWebsite\\\\n\\\\nGoogle Scholars Link\\\\n\\\\nEmail dettmers@cmu.edu\\\\n\\\\nDepartment  \\\\nMachine Learning Department  \\\\nComputer Science Department\\\\n\\\\nComputer Science Department\\\\n---------------------------\\\\n\\\\nCarnegie Mellon University\\\\n\\\\n5000 Forbes Avenue\\\\n\\\\nPittsburgh, PA 15213\\\\n\\\\nFax: 412-268-5576\\\\n\\\\n            \\\\n\\\\nImage 3: Carnegie Mellon University School of Computer Science [...] About\\\\n    \\\\n    ### Back to Main Menu\\\\n    \\\\n    ### About Main page\\\\n    \\\\n       About  \\\\n        Related links\\\\n           Events\\\\n           News\\\\n           Key Contacts\\\\n           History\\\\n           Sitemap\\\\n       Employment  \\\\n        Related links\\\\n           Faculty Hiring\\\\n           Staff Hiring\\\\n       Marketing & Communications  \\\\n        Related links\\\\n           SCS Marketing & Communications\\\\n           Partnerships\\\\n           Employer Recruiting\\\\n           CMU Marketing & Communications\", \"score\": 0.6729558}, {\"title\": \"Tim Dettmers - Quora\", \"url\": \"https://www.quora.com/profile/Tim-Dettmers-1\", \"content\": \"Kernel methods are practically obsolete, but their math still shines on and is worth a look. Kernel methods are not only practically obsolete due to their inferior predictive performance when compared to deep learning, but also because they require a lot of feature engineering and because they are seâ€¦\\\\n\\\\n(more)\\\\n\\\\nImage 8: Profile photo for Tim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nPhD Student at University of Washington (2018â€“present)\\\\n\\\\nÂ·9y [...] PhD Student at University of Washington (2018â€“present)\\\\n\\\\nÂ·9y\\\\n\\\\nHow feasible would it be to build a deep learning platform on a distributed heterogeneous environment over the Internet (e.g. desktop/mobile grid)? [...] Image 9: Profile photo for Tim Dettmers\\\\n\\\\nTim Dettmers\\\\n\\\\nPhD Student at University of Washington (2018â€“present)\\\\n\\\\nÂ·9y\\\\n\\\\nHow can I use a deep neural network trained model by multiple GPUs?\\\\n\\\\nIf you want to use multiple GPUs you should use Torch7 along with the Facebook research libraries. The parallel implementations of Torch7 provide good speed, without any loss of accuracy and are very easy to use. There are other libraries that make use of parallelism, but they often have poor APIs oâ€¦\\\\n\\\\n(more)\", \"score\": 0.57666177}]', name='tavily_search_results_json', id='173d0d23-b97f-4ce9-83b3-135fcd7e0aaf', tool_call_id='call_PHRUAJvjHyI9bPdAMeAmT4t5', artifact={'query': 'Tim Dettmers', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://ai2050.schmidtsciences.org/fellow/tim-dettmers/', 'title': 'Tim Dettmers - AI2050 - Schmidt Sciences', 'content': 'Tim Dettmers is an Assistant Professor at Carnegie Mellon University and a Research Scientist at the Allen Institute for AI, and his research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. He has won oral, spotlight, and best paper awards at conferences [...] such as ICLR and NeurIPS. He created the bitsandbytes library for efficient deep learning, which is growing at 2.2 million installations per month, and received Google Open Source and PyTorch Foundation awards. [...] AI models like ChatGPT work well for general use but fail in specialized expert domains, such as the medical sciences. To make AI models work in expert domains, one must adapt them, which is costly and requires significant AI expertise. This project overcomes these cost and expertise barriers through two new approaches: (1) use AI models themselves to perform the AI model adaptation process automatically; (2) make the adaptation process cheap so it can be run on regular consumer hardware. With', 'score': 0.9245858, 'raw_content': None}, {'url': 'https://cse.gatech.edu/events/2024/02/20/cse-faculty-candidate-seminar-tim-dettmers', 'title': 'CSE Faculty Candidate Seminar - Tim Dettmers', 'content': 'Bio:Tim Dettmersâ€™s research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. These methods enable many more people to use, adapt, or train foundation models without affecting the quality of AI predictions or generations. He is a PhD candidate at the [...] News and Events\\n       News\\n       Upcoming Events\\n       Calendar\\n       CSE Biweekly Roundup\\n       Analyzer\\n       2024 Annual Brief.pdf)\\n\\n\\n\\nSearch\\n------\\n\\nSearch \\n\\nBreadcrumb\\n----------\\n\\n1.   Home\\n2.   Events\\n\\nUpcoming Events\\n---------------\\n\\nCSE Faculty Candidate Seminar - Tim Dettmers\\n============================================\\n\\nImage 2: Tim Dettmers.png\\n\\nName:Tim Dettmers, Ph.D. student at University of Washington\\n\\nDate:Tuesday, February 20, 2024 at 11:00 am [...] University of Washington and has won oral, spotlight, and best paper awards at conferences such as ICLR and NeurIPS. He created the bitsandbytes library for efficient deep learning, which is growing at 1.4 million installations per month and received Google Open Source and PyTorch Foundation awards.', 'score': 0.70531887, 'raw_content': None}, {'url': 'https://timdettmers.com/about/', 'title': 'About Me - Tim Dettmers', 'content': 'Research Interests  \\nPublications  \\nAwards & Honors  \\nService\\n\\nGoogle Scholar\\n\\nfirstname.lastname@gmail.com\\n\\nImage 1I am a research scientist at the Allen Institute for Artificial Intelligence (Ai2) and an incoming Assistant Professor at Carnegie Mellon University (CMU). I am the creator and maintainer of bitsandbytes. [...] I have a PhD from University of Washington advised by Luke Zettlemoyer working on efficient deep learning at the intersection between machine learning, natural language processing, and computer systems with a focus on quantization and sparsity. My main research goal is to empower everyone to make AI their own. I do this by making large models accessible through my research (QLoRA, LLM.int8(), k-bit inference scaling laws, Petals, SWARM) and by developing software that makes it easy to use my [...] About Me â€” Tim Dettmers\\n===============  \\n\\nSkip links\\n----------\\n\\n   Skip to primary navigation\\n   Skip to content\\n   Skip to primary sidebar\\n\\nTim Dettmers\\n\\nMaking deep learning accessible.\\n\\nHeader Right\\n------------\\n\\n### Blog Posts Topics\\n\\n   Academia (4)\\n       PhD Life (3)\\n   Deep Learning (7)\\n   Hardware (8)\\n   Science (4)\\n       Neuroscience (1)\\n\\nMain navigation\\n---------------\\n\\n   Blog\\n       Deep Learning\\n       Hardware\\n       Neuroscience\\n   Publications\\n   About Me\\n\\nAbout Me\\n========', 'score': 0.68318754, 'raw_content': None}, {'url': 'https://csd.cmu.edu/people/faculty/tim-dettmers', 'title': 'Tim Dettmers | Carnegie Mellon University Computer Science ...', 'content': 'X\\n\\nBreadcrumb\\n----------\\n\\n1.  Home\\n2.  People\\n3.  Faculty\\n4.  Tim Dettmers\\n\\nTim Dettmers\\n============\\n\\nImage 2: Tim DettmersAssistant Professor\\n\\nWebsite\\n\\nGoogle Scholars Link\\n\\nEmail dettmers@cmu.edu\\n\\nDepartment  \\nMachine Learning Department  \\nComputer Science Department\\n\\nComputer Science Department\\n---------------------------\\n\\nCarnegie Mellon University\\n\\n5000 Forbes Avenue\\n\\nPittsburgh, PA 15213\\n\\nFax: 412-268-5576\\n\\n            \\n\\nImage 3: Carnegie Mellon University School of Computer Science [...] About\\n    \\n    ### Back to Main Menu\\n    \\n    ### About Main page\\n    \\n       About  \\n        Related links\\n           Events\\n           News\\n           Key Contacts\\n           History\\n           Sitemap\\n       Employment  \\n        Related links\\n           Faculty Hiring\\n           Staff Hiring\\n       Marketing & Communications  \\n        Related links\\n           SCS Marketing & Communications\\n           Partnerships\\n           Employer Recruiting\\n           CMU Marketing & Communications', 'score': 0.6729558, 'raw_content': None}, {'url': 'https://www.quora.com/profile/Tim-Dettmers-1', 'title': 'Tim Dettmers - Quora', 'content': 'Kernel methods are practically obsolete, but their math still shines on and is worth a look. Kernel methods are not only practically obsolete due to their inferior predictive performance when compared to deep learning, but also because they require a lot of feature engineering and because they are seâ€¦\\n\\n(more)\\n\\nImage 8: Profile photo for Tim Dettmers\\n\\nTim Dettmers\\n\\nPhD Student at University of Washington (2018â€“present)\\n\\nÂ·9y [...] PhD Student at University of Washington (2018â€“present)\\n\\nÂ·9y\\n\\nHow feasible would it be to build a deep learning platform on a distributed heterogeneous environment over the Internet (e.g. desktop/mobile grid)? [...] Image 9: Profile photo for Tim Dettmers\\n\\nTim Dettmers\\n\\nPhD Student at University of Washington (2018â€“present)\\n\\nÂ·9y\\n\\nHow can I use a deep neural network trained model by multiple GPUs?\\n\\nIf you want to use multiple GPUs you should use Torch7 along with the Facebook research libraries. The parallel implementations of Torch7 provide good speed, without any loss of accuracy and are very easy to use. There are other libraries that make use of parallelism, but they often have poor APIs oâ€¦\\n\\n(more)', 'score': 0.57666177, 'raw_content': None}], 'response_time': 2.11}), ToolMessage(content='[{\"title\": \"Attention Mechanisms and Their Applications to Complex Systems\", \"url\": \"https://pmc.ncbi.nlm.nih.gov/articles/PMC7996841/\", \"content\": \"Generally formulated, attention in machine learning is a sequential process in which a learning task is guided by a set of elements of the input source (or memory). This is achieved by integrating the attention value into the task. [...] Attention mechanisms have provided and will provide a paradigm shift in machine learning. Specifically, this change is from traditional large-scale vector transformations to more conscious processes (i.e., that focus only on a set of elements), e.g., decomposing a problem into a sequence of attention based reasoning tasks [13,30,31,32,33,34]. [...] Attention mechanisms have provided and will provide a paradigm shift in machine learning [11,12]. These mechanisms allow a model to focus only on a set of elements and to decompose a problem into a sequence of attention based reasoning tasks . Moreover, they can be applied to model complex systems in a flexible and promising way. When it comes to their application, information processing in the system and internal structure are crucial.\", \"score\": 0.9581988}, {\"title\": \"Attention (machine learning) - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Attention_(machine_learning)\", \"content\": \"Attention is a machine learning method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \\\\\"soft\\\\\" weights assigned to each word in a sentence. More generally, attention encodes vectors called tokenembeddings across a fixed-width sequence that can range from tens to millions of tokens in size. [...] Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\\\\n\\\\nHistory\\\\n------- [...] Attention(A Q,B K,B V)=A Attention(Q,K,V){\\\\\\\\displaystyle {\\\\\\\\text{Attention}}(\\\\\\\\mathbf {A} \\\\\\\\mathbf {Q} ,\\\\\\\\mathbf {B} \\\\\\\\mathbf {K} ,\\\\\\\\mathbf {B} \\\\\\\\mathbf {V} )=\\\\\\\\mathbf {A} \\\\\\\\,{\\\\\\\\text{Attention}}(\\\\\\\\mathbf {Q} ,\\\\\\\\mathbf {K} ,\\\\\\\\mathbf {V} )}Image 31: {\\\\\\\\displaystyle {\\\\\\\\text{Attention}}(\\\\\\\\mathbf {A} \\\\\\\\mathbf {Q} ,\\\\\\\\mathbf {B} \\\\\\\\mathbf {K} ,\\\\\\\\mathbf {B} \\\\\\\\mathbf {V} )=\\\\\\\\mathbf {A} \\\\\\\\,{\\\\\\\\text{Attention}}(\\\\\\\\mathbf {Q} ,\\\\\\\\mathbf {K} ,\\\\\\\\mathbf {V} )}\", \"score\": 0.94956195}, {\"title\": \"Attention mechanism: Overview - YouTube\", \"url\": \"https://www.youtube.com/watch?v=fjJOgb-E41w\", \"content\": \"# Attention mechanism: Overview\\\\n\\\\nGoogle Cloud Tech\\\\n2877 likes\\\\n191914 views\\\\n5 Jun 2023\\\\nThis video introduces you to the attention mechanism, a powerful technique that allows neural networks to focus on specific parts of an input sequence. Attention is used to improve the performance of a variety of machine learning tasks, including machine translation, text summarization, and question answering.\", \"score\": 0.83932644}, {\"title\": \"Unpacking the Power of Attention Mechanisms in Deep Learning\", \"url\": \"https://viso.ai/deep-learning/attention-mechanisms/\", \"content\": \"Attention mechanisms represent advancements in machine learning and computer vision, enabling models to prioritize relevant information for better performance. As research progresses, attention mechanisms will further enhance the capabilities and interpretability of deep learning models.\\\\n\\\\nTo continue learning about the world of computer vision, check out our other blogs: [...] Addressing Data Processing\\\\n--------------------------\\\\n\\\\nAttention mechanisms address a critical challenge in AI: the efficient processing of vast and complex data sets. By enabling models to selectively weigh the importance of different input features, they improve both accuracy and efficiency. This makes these models perform better for tasks such as image recognition, text translation, and speech recognition. [...] Attention mechanisms allow artificial intelligence (AI) models to dynamically focus on individual elements within visual data. This mimics the way humans concentrate on specific visual elements at a time. This enhances the interpretability of AI systems for applications in computer vision and natural language processing (NLP).\", \"score\": 0.81665546}, {\"title\": \"What are Attention Mechanisms in Deep Learning? - freeCodeCamp\", \"url\": \"https://www.freecodecamp.org/news/what-are-attention-mechanisms-in-deep-learning/\", \"content\": \"The attention mechanism has found applications in artificial intelligence and deep learning in a wide range of domains. Here are some notable scenarios:\\\\n\\\\nMachine Translation: Attention mechanisms enhanced the quality of machine translation systems dramatically. They enable models to concentrate on certain words or phrases in the source language when producing the corresponding terms in the target language, hence boosting translation accuracy. [...] Attention was originally employed in neural machine translation to assist the model in focusing on the most significant words or phrases in a sentence when translating it into another language. Since then, attention has become widely used in a variety of deep learning applications, including computer vision, speech recognition, and recommender systems.\\\\n\\\\n## How Does the Attention Mechanism Work? [...] In a nutshell, attention mechanisms assist machines in focusing on what is important in data, allowing them to perform better at tasks such as language processing, image recognition, and others. Itâ€™s more than simply a technical change â€“ itâ€™s a significant player in the realm of artificial intelligence, bringing up intriguing possibilities for smarter and more efficient systems.\\\\n\\\\nOyedele Tioluwani\\\\n\\\\nPython developer\\\\nStudying Mechanical Engineering\\\\nMachine Learning enthusiast\", \"score\": 0.81595254}]', name='tavily_search_results_json', id='96ce0718-de93-48f7-8a9f-ada917607a16', tool_call_id='call_8nllw3FyCKCRMjuDsBtwabTw', artifact={'query': 'Attention in machine learning', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC7996841/', 'title': 'Attention Mechanisms and Their Applications to Complex Systems', 'content': 'Generally formulated, attention in machine learning is a sequential process in which a learning task is guided by a set of elements of the input source (or memory). This is achieved by integrating the attention value into the task. [...] Attention mechanisms have provided and will provide a paradigm shift in machine learning. Specifically, this change is from traditional large-scale vector transformations to more conscious processes (i.e., that focus only on a set of elements), e.g., decomposing a problem into a sequence of attention based reasoning tasks [13,30,31,32,33,34]. [...] Attention mechanisms have provided and will provide a paradigm shift in machine learning [11,12]. These mechanisms allow a model to focus only on a set of elements and to decompose a problem into a sequence of attention based reasoning tasks . Moreover, they can be applied to model complex systems in a flexible and promising way. When it comes to their application, information processing in the system and internal structure are crucial.', 'score': 0.9581988, 'raw_content': None}, {'url': 'https://en.wikipedia.org/wiki/Attention_(machine_learning)', 'title': 'Attention (machine learning) - Wikipedia', 'content': 'Attention is a machine learning method that determines the importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by \"soft\" weights assigned to each word in a sentence. More generally, attention encodes vectors called tokenembeddings across a fixed-width sequence that can range from tens to millions of tokens in size. [...] Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state.\\n\\nHistory\\n------- [...] Attention(A Q,B K,B V)=A Attention(Q,K,V){\\\\displaystyle {\\\\text{Attention}}(\\\\mathbf {A} \\\\mathbf {Q} ,\\\\mathbf {B} \\\\mathbf {K} ,\\\\mathbf {B} \\\\mathbf {V} )=\\\\mathbf {A} \\\\,{\\\\text{Attention}}(\\\\mathbf {Q} ,\\\\mathbf {K} ,\\\\mathbf {V} )}Image 31: {\\\\displaystyle {\\\\text{Attention}}(\\\\mathbf {A} \\\\mathbf {Q} ,\\\\mathbf {B} \\\\mathbf {K} ,\\\\mathbf {B} \\\\mathbf {V} )=\\\\mathbf {A} \\\\,{\\\\text{Attention}}(\\\\mathbf {Q} ,\\\\mathbf {K} ,\\\\mathbf {V} )}', 'score': 0.94956195, 'raw_content': None}, {'url': 'https://www.youtube.com/watch?v=fjJOgb-E41w', 'title': 'Attention mechanism: Overview - YouTube', 'content': '# Attention mechanism: Overview\\n\\nGoogle Cloud Tech\\n2877 likes\\n191914 views\\n5 Jun 2023\\nThis video introduces you to the attention mechanism, a powerful technique that allows neural networks to focus on specific parts of an input sequence. Attention is used to improve the performance of a variety of machine learning tasks, including machine translation, text summarization, and question answering.', 'score': 0.83932644, 'raw_content': None}, {'url': 'https://viso.ai/deep-learning/attention-mechanisms/', 'title': 'Unpacking the Power of Attention Mechanisms in Deep Learning', 'content': 'Attention mechanisms represent advancements in machine learning and computer vision, enabling models to prioritize relevant information for better performance. As research progresses, attention mechanisms will further enhance the capabilities and interpretability of deep learning models.\\n\\nTo continue learning about the world of computer vision, check out our other blogs: [...] Addressing Data Processing\\n--------------------------\\n\\nAttention mechanisms address a critical challenge in AI: the efficient processing of vast and complex data sets. By enabling models to selectively weigh the importance of different input features, they improve both accuracy and efficiency. This makes these models perform better for tasks such as image recognition, text translation, and speech recognition. [...] Attention mechanisms allow artificial intelligence (AI) models to dynamically focus on individual elements within visual data. This mimics the way humans concentrate on specific visual elements at a time. This enhances the interpretability of AI systems for applications in computer vision and natural language processing (NLP).', 'score': 0.81665546, 'raw_content': None}, {'url': 'https://www.freecodecamp.org/news/what-are-attention-mechanisms-in-deep-learning/', 'title': 'What are Attention Mechanisms in Deep Learning? - freeCodeCamp', 'content': 'The attention mechanism has found applications in artificial intelligence and deep learning in a wide range of domains. Here are some notable scenarios:\\n\\nMachine Translation: Attention mechanisms enhanced the quality of machine translation systems dramatically. They enable models to concentrate on certain words or phrases in the source language when producing the corresponding terms in the target language, hence boosting translation accuracy. [...] Attention was originally employed in neural machine translation to assist the model in focusing on the most significant words or phrases in a sentence when translating it into another language. Since then, attention has become widely used in a variety of deep learning applications, including computer vision, speech recognition, and recommender systems.\\n\\n## How Does the Attention Mechanism Work? [...] In a nutshell, attention mechanisms assist machines in focusing on what is important in data, allowing them to perform better at tasks such as language processing, image recognition, and others. Itâ€™s more than simply a technical change â€“ itâ€™s a significant player in the realm of artificial intelligence, bringing up intriguing possibilities for smarter and more efficient systems.\\n\\nOyedele Tioluwani\\n\\nPython developer\\nStudying Mechanical Engineering\\nMachine Learning enthusiast', 'score': 0.81595254, 'raw_content': None}], 'response_time': 2.44})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"Here's a summary of the information I found:\\n\\n1. **LoRA (Low-Rank Adaptation)**: LoRA is a technique used to adapt large machine learning models to specific tasks or domains efficiently. It works by adding lightweight, low-rank matrices to the original model, which contain new weights. During fine-tuning, only these matrices are trained, not the entire model. This approach reduces computational requirements and training time, making it easier and faster to customize large models for particular applications. LoRA is especially useful for fine-tuning large language models and has been shown to enable quick and resource-efficient adaptation. [Learn more](https://www.cloudflare.com/learning/ai/what-is-lora/)\\n\\n2. **Tim Dettmers**: Tim Dettmers is an Assistant Professor at Carnegie Mellon University and a Research Scientist at the Allen Institute for AI. His research focuses on making foundation models like ChatGPT more accessible by reducing their resource requirements. He has developed algorithms and systems for memory-efficient, fast, and affordable deep learning. He is also the creator of the bitsandbytes library, which is widely used for efficient deep learning. Dettmers aims to democratize AI by making large models easier to use, adapt, and deploy, especially in specialized domains. [More about him](https://ai2050.schmidtsciences.org/fellow/tim-dettmers/)\\n\\n3. **Attention in Machine Learning**: Attention mechanisms are techniques that allow models to focus on specific parts of the input data, assigning different importance weights to different elements. Inspired by human cognition, attention helps models process sequences more effectively, such as words in a sentence or regions in an image. It has revolutionized many areas of AI, including natural language processing and computer vision, by enabling models to decompose complex tasks into focused reasoning steps. Attention mechanisms improve both the performance and interpretability of AI systems. [More details](https://en.wikipedia.org/wiki/Attention_(machine_learning))\\n\\nWould you like more detailed information on any of these topics?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 413, 'prompt_tokens': 4450, 'total_tokens': 4863, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': None, 'id': 'chatcmpl-BtU1tL0oX8Vd0nUVFU7pmAyWyJVln', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d74bd015-9fb8-432f-b9ca-991db267d0e3-0', usage_metadata={'input_tokens': 4450, 'output_tokens': 413, 'total_tokens': 4863, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "d847426e-71b3-47e6-b1ae-351a78d68d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt engineering is the process of designing and refining input prompts to effectively communicate with AI language models, such as GPT, to obtain desired responses. It involves crafting prompts that are clear, specific, and contextually appropriate to guide the model's output in a useful and accurate manner.\n",
            "\n",
            "Prompt engineering has gained significant prominence with the rise of large language models (LLMs) like GPT-3, which were released around 2020-2021. As these models became more widely adopted, the importance of effectively interacting with them through well-designed prompts became evident, leading to the emergence of prompt engineering as a specialized skill.\n",
            "\n",
            "Would you like me to find more detailed information or specific milestones related to the development of prompt engineering?\n",
            "\n",
            "\n",
            "\n",
            "Retrieval-Augmented Generation (RAG) is an AI framework that enhances the capabilities of large language models (LLMs) by allowing them to retrieve and incorporate relevant external information before generating responses. This approach improves the accuracy, relevance, and contextual understanding of AI-generated content by grounding the model on external sources of knowledge, such as documents, databases, or knowledge bases.\n",
            "\n",
            "RAG was first introduced in 2020 by researchers at Meta (formerly Facebook) as a way to give LLMs access to information beyond their training data, making them more adaptable to new and specific knowledge without the need for retraining. Since then, it has gained traction and is increasingly used in AI applications to improve performance and reliability.\n",
            "\n",
            "In summary:\n",
            "- **What:** A method to improve LLM responses by retrieving external data.\n",
            "- **When:** First introduced in 2020.\n",
            "- **Why:** To make AI responses more accurate, up-to-date, and contextually relevant.\n",
            "\n",
            "Would you like more detailed information on its development or applications?\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a machine learning technique used to adapt a pre-trained model to a specific task or dataset. Instead of training a model from scratch, which can be computationally expensive and require large amounts of data, fine-tuning involves taking an existing model that has already learned general features and adjusting its parameters slightly to perform well on a particular problem. This process typically involves continuing the training process on a smaller, task-specific dataset, allowing the model to specialize without losing the general knowledge it has already acquired.\n",
            "\n",
            "Fine-tuning has been a common practice in machine learning for many years, especially in fields like natural language processing and computer vision. However, it gained significant prominence and broke onto the scene in the context of large pre-trained models around the mid-2010s. The release of models like BERT (2018) and GPT (2018 for GPT-1, with GPT-2 in 2019, and GPT-3 in 2020) popularized the approach, demonstrating how pre-trained models could be effectively adapted to a wide range of specific tasks through fine-tuning.\n",
            "\n",
            "Would you like me to find more detailed historical information or specific milestones related to the emergence of fine-tuning?\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents are systems that utilize large language models (LLMs) to perform tasks, make decisions, or interact with users in a human-like manner. These agents leverage the capabilities of LLMs such as understanding natural language, generating coherent responses, and even reasoning to some extent. They are used in various applications including chatbots, virtual assistants, and automated content creation.\n",
            "\n",
            "The concept of LLM-based agents has gained significant attention and development in recent years, particularly around 2023. They have evolved from simple AI systems to complex, multi-agent ecosystems capable of collaboration, negotiation, and adaptive learning. The research and practical implementations of LLM-based agents have been rapidly expanding, marking a notable milestone in the field of artificial intelligence.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
